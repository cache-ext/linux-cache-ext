{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from subprocess import run\n",
    "import multiprocessing as mp\n",
    "\n",
    "from copy import deepcopy\n",
    "from ruamel.yaml import YAML\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from typing import List, Dict, Any, Tuple\n",
    "\n",
    "\n",
    "yaml = YAML()\n",
    "random.seed(42)\n",
    "IO_BUFFER_SIZE = 4 * 2**20  # 4 MiB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "KiB = 2**10\n",
    "MiB = 2**20\n",
    "GiB = 2**30\n",
    "\n",
    "\n",
    "def download_to_file(url: str, file: str):\n",
    "    with requests.get(url, stream=True) as r:\n",
    "        r.raise_for_status()\n",
    "        with open(file, 'wb') as f:\n",
    "            for chunk in r.iter_content(chunk_size=8192):\n",
    "                f.write(chunk)\n",
    "\n",
    "\n",
    "def format_number(num: int) -> str:\n",
    "    \"\"\"Format a number with commas.\"\"\"\n",
    "    return \"{:,}\".format(num)\n",
    "\n",
    "\n",
    "def format_bytes(num_bytes: int) -> str:\n",
    "    \"\"\"Format bytes in human-readable form.\"\"\"\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if num_bytes < 1024:\n",
    "            return f\"{num_bytes:.2f} {unit}\"\n",
    "        num_bytes /= 1024\n",
    "    return f\"{num_bytes:.2f} PB\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "# Previous Inputs:\n",
    "# - cluster_17, 0, 12M rows\n",
    "# - cluster_18, 0, 20M rows\n",
    "# - cluster_34, 0, 20M rows\n",
    "# - cluster_52, 0, 20M rows\n",
    "\n",
    "# Trace file (zstd compressed)\n",
    "cluster_name: str = \"cluster18\"\n",
    "num_rows_to_keep: int = 20 * 10**6\n",
    "delete_trace_file: bool = False\n",
    "force: bool = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part = 0\n",
    "trace_file = \"/mydata/twitter_traces/%s\" % cluster_name\n",
    "compressed_trace_file = \"%s.sort.zst\" % trace_file\n",
    "\n",
    "init_workload_file = trace_file + \"_init.txt\"\n",
    "bench_workload_file = trace_file + \"_bench.txt\"\n",
    "\n",
    "# Trace analysis\n",
    "analyze_trace = False\n",
    "\n",
    "create_data_dir = True\n",
    "myycsb_init_yaml_file = \"/mydata/My-YCSB/leveldb/config/twitter_%s_init.yaml\" % cluster_name\n",
    "myycsb_bench_yaml_file = \"/mydata/My-YCSB/leveldb/config/twitter_%s_bench.yaml\" % cluster_name\n",
    "data_dir = \"/mydata/leveldb_twitter_%s_db\" % cluster_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading trace file\n"
     ]
    }
   ],
   "source": [
    "download_tmpl = \"https://ftp.pdl.cmu.edu/pub/datasets/twemcacheWorkload/open_source/%s.sort.zst\"\n",
    "if not os.path.exists(compressed_trace_file):\n",
    "    print(\"Downloading trace file\")\n",
    "    download_to_file(download_tmpl % cluster_name, compressed_trace_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace rows:  20000000\n"
     ]
    }
   ],
   "source": [
    "# Parse trace file into pandas dataframe\n",
    "# Line format is comma separated:\n",
    "# timestamp, key, key size, value size, client id, operation, ttl\n",
    "columns = [\"timestamp\", \"key\", \"key_size\", \"value_size\", \"client_id\", \"op_type\", \"ttl\"]\n",
    "trace = pd.read_csv(compressed_trace_file, names=columns, index_col=False, compression=\"zstd\", nrows=num_rows_to_keep, on_bad_lines=\"warn\")\n",
    "print(\"Trace rows: \", len(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value size:  34\n",
      "Median value size:  28\n",
      "Mean key size:  15\n",
      "Median key size:  15\n",
      "Key size:  15\n",
      "Value size:  34\n"
     ]
    }
   ],
   "source": [
    "# Print mean and median value size. Make sure keys first.\n",
    "# Calculate mean and median value sizes\n",
    "mean_value_size = trace.groupby(\"key\")[\"value_size\"].mean().mean()\n",
    "median_value_size = trace.groupby(\"key\")[\"value_size\"].median().median()\n",
    "\n",
    "print(\"Mean value size: \", int(mean_value_size))\n",
    "print(\"Median value size: \", int(median_value_size))\n",
    "\n",
    "# Convert key_size to numeric, dropping any non-numeric values\n",
    "temp_trace = trace.copy()\n",
    "temp_trace[\"key_size\"] = pd.to_numeric(temp_trace[\"key_size\"], errors=\"coerce\")\n",
    "# Keep only rows where key_size is not null (was successfully converted to numeric)\n",
    "temp_trace = temp_trace[temp_trace[\"key_size\"].notna()]\n",
    "\n",
    "\n",
    "# Calculate mean and median key sizes\n",
    "mean_key_size = temp_trace.groupby(\"key\")[\"key_size\"].mean().mean()\n",
    "median_key_size = temp_trace.groupby(\"key\")[\"key_size\"].median().median()\n",
    "\n",
    "print(\"Mean key size: \", int(mean_key_size))\n",
    "print(\"Median key size: \", int(median_key_size))\n",
    "\n",
    "key_size = int(mean_key_size)\n",
    "value_size = int(mean_value_size)\n",
    "\n",
    "print(\"Key size: \", key_size)\n",
    "print(\"Value size: \", value_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working set size:  54.62 MB\n"
     ]
    }
   ],
   "source": [
    "# How big is the working set?\n",
    "num_unique_keys = trace[\"key\"].nunique()\n",
    "wss = num_unique_keys * (key_size + value_size)\n",
    "print(\"Working set size: \", format_bytes(wss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps are ordered: True\n",
      "Trace has 20,000,000 rows, no truncation needed\n"
     ]
    }
   ],
   "source": [
    "# Check if timestamp column is ordered\n",
    "# Check if timestamps are monotonically increasing\n",
    "is_ordered = trace[\"timestamp\"].is_monotonic_increasing\n",
    "print(\"Timestamps are ordered:\", is_ordered)\n",
    "\n",
    "if not is_ordered:\n",
    "    raise ValueError(\"Timestamps are not ordered\")\n",
    "\n",
    "# Keep only first num_rows_to_keep rows\n",
    "if len(trace) > num_rows_to_keep:\n",
    "    print(f\"Keeping first {num_rows_to_keep:,} rows out of {len(trace):,} total rows\")\n",
    "    trace = trace.head(num_rows_to_keep)\n",
    "else:\n",
    "    print(f\"Trace has {len(trace):,} rows, no truncation needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_trace:\n",
    "    # Print a list of value sizes and their counts in decreasing order\n",
    "    value_size_counts = trace[\"value_size\"].value_counts()\n",
    "    print(\"Value size counts:\")\n",
    "    print(value_size_counts)\n",
    "\n",
    "    key_size_counts = trace[\"key_size\"].value_counts()\n",
    "    print(\"Key size counts:\")\n",
    "    print(key_size_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "if analyze_trace:\n",
    "    # How many lines is the trace?\n",
    "    print(\"Number of lines in trace: %d\" % len(trace))\n",
    "\n",
    "    # What is the proportion of read and write operations?\n",
    "    read_ops = len(trace[trace[\"op_type\"] == \"get\"])\n",
    "    write_ops = len(trace[trace[\"op_type\"] == \"set\"])\n",
    "    print(\"Read operations: %d\" % read_ops)\n",
    "    print(\"Write operations: %d\" % write_ops)\n",
    "    print(\"Read proportion: %.2f%%\" % (100 * read_ops / (read_ops + write_ops)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing keys to file\n"
     ]
    }
   ],
   "source": [
    "# Prefix pad keys with zeros to make them the same length.\n",
    "# Alternatively, trim keys to a fixed length.\n",
    "def pad_key(key: str, target_size: int) -> str:\n",
    "    # If key is longer than target size, truncate it\n",
    "    if len(key) > target_size:\n",
    "        return key[:target_size]\n",
    "    # If key is shorter than target size, pad with zeros\n",
    "    return key.zfill(target_size)\n",
    "\n",
    "\n",
    "# Pad trace keys to a fixed size\n",
    "trace[\"key\"] = trace[\"key\"].apply(lambda x: pad_key(x, key_size))\n",
    "num_unique_keys = trace[\"key\"].nunique()\n",
    "\n",
    "def generate_init_workload(trace: pd.DataFrame, filename: str) -> None:\n",
    "    # Construct the init workload file\n",
    "    # Each line is a key\n",
    "    # All keys and values are the same size\n",
    "    trace_keys = list(trace[\"key\"].unique())\n",
    "\n",
    "    # Permuate the keys\n",
    "    random.shuffle(trace_keys)\n",
    "\n",
    "    # Write keys to file\n",
    "    print(\"Writing keys to file\")\n",
    "    with open(filename, \"w\", buffering=IO_BUFFER_SIZE) as f:\n",
    "        for i, key in enumerate(trace_keys):\n",
    "            if i > 0:\n",
    "                f.write(\"\\n\")\n",
    "            f.write(key)\n",
    "\n",
    "if not os.path.exists(init_workload_file) or force:\n",
    "    print(\"Generating init workload file\")\n",
    "    generate_init_workload(trace, init_workload_file)\n",
    "else:\n",
    "    print(\"Init workload file already exists\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating bench workload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_413935/3624191965.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for _, row in tqdm(trace.iterrows(), total=len(trace), desc=\"Writing operations\"):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64395cc99cf24a70a4677db1682e9578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing operations:   0%|          | 0/20000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating bench workload\n"
     ]
    }
   ],
   "source": [
    "# Now generate the bench workload file\n",
    "# Each line is an operation followed by a key (space separated)\n",
    "# Operations are get or insert\n",
    "\n",
    "def generate_bench_workload(trace: pd.DataFrame, filename: str) -> None:\n",
    "    # Construct the bench workload file\n",
    "    # Each line is an operation followed by a key (space separated)\n",
    "    print(\"Generating bench workload\")\n",
    "\n",
    "    with open(filename, \"w\", buffering=IO_BUFFER_SIZE) as f:\n",
    "        # Iterate through trace rows with progress bar\n",
    "        for i, (_, row) in enumerate(tqdm(trace.iterrows(), total=len(trace), desc=\"Writing operations\")):\n",
    "            key = row[\"key\"]\n",
    "\n",
    "            # Add newline before all lines except the first\n",
    "            if i > 0:\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "            # Write operation and key\n",
    "            if row[\"op_type\"] in [\"get\", \"gets\"]:\n",
    "                f.write(\"get \" + key)\n",
    "            elif row[\"op_type\"] == \"cas\":\n",
    "                f.write(\"update \" + key)\n",
    "            else:\n",
    "                f.write(\"insert \" + key)\n",
    "\n",
    "    print(\"Done generating bench workload\")\n",
    "\n",
    "\n",
    "if not os.path.exists(bench_workload_file) or force:\n",
    "    print(\"Generating bench workload file\")\n",
    "    generate_bench_workload(trace, bench_workload_file)\n",
    "else:\n",
    "    print(\"Bench workload file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a summary of the trace that might be useful later\n",
    "summary_file = trace_file + \"_summary.txt\"\n",
    "summary = {\n",
    "    \"num_rows\": len(trace),\n",
    "    \"key_size\": key_size,\n",
    "    \"value_size\": value_size,\n",
    "    \"num_unique_keys\": num_unique_keys,\n",
    "    \"working_set_size\": wss\n",
    "}\n",
    "\n",
    "if not os.path.exists(summary_file) or force:\n",
    "    print(\"Writing summary file\")\n",
    "    with open(summary_file, \"w\") as f:\n",
    "        yaml.dump(summary, f)\n",
    "else:\n",
    "    print(\"Summary file already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "libcachesim_dir = \"/mydata/libCacheSim\"\n",
    "trace_analyzer = os.path.join(libcachesim_dir, \"_build/bin/traceAnalyzer\")\n",
    "\n",
    "if not os.path.exists(trace_analyzer):\n",
    "    print(\"libCacheSim traceAnalyzer not found. Skipping trace analysis.\")\n",
    "elif analyze_trace:\n",
    "    cmd = [trace_analyzer, trace_file, \"csv\", \"--common\", \"-t\", \"time-col=1, obj-id-col=2, obj-size-col=4, delimiter=,, has-header=false\"]\n",
    "    print(\"Running trace analysis\")\n",
    "    run(cmd, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data directory created\n",
      "Creating My-YCSB init YAML file\n",
      "Running My-YCSB init\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LevelDBFactory: data_dir: /mydata/leveldb_twitter_cluster18_db, print_stats: 1\n",
      "InitTraceWorkload: trace_path=/mydata/twitter_traces/cluster18_init.txt, trace_type=twitter_init\n",
      "Number of lines in trace file: 1168683\n",
      "Key size: 16, Value size: 35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialization (trace) (epoch 0, progress 0.00%): UPDATE throughput 0.00 ops/sec, INSERT throughput 0.00 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 0.00 ops/sec\n",
      "Initialization (trace) (epoch 1, progress 8.19%): UPDATE throughput 0.00 ops/sec, INSERT throughput 95737.14 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 95737.14 ops/sec\n",
      "Initialization (trace) (epoch 2, progress 16.43%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96221.03 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96221.03 ops/sec\n",
      "Initialization (trace) (epoch 3, progress 24.65%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96115.37 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96115.37 ops/sec\n",
      "Initialization (trace) (epoch 4, progress 32.87%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96056.70 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96056.70 ops/sec\n",
      "Initialization (trace) (epoch 5, progress 41.13%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96479.86 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96479.86 ops/sec\n",
      "Initialization (trace) (epoch 6, progress 49.34%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96005.47 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96005.47 ops/sec\n",
      "Initialization (trace) (epoch 7, progress 57.60%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96461.57 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96461.57 ops/sec\n",
      "Initialization (trace) (epoch 8, progress 65.85%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96463.67 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96463.67 ops/sec\n",
      "Initialization (trace) (epoch 9, progress 74.09%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96324.39 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96324.39 ops/sec\n",
      "Initialization (trace) (epoch 10, progress 82.13%): UPDATE throughput 0.00 ops/sec, INSERT throughput 93914.20 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 93914.20 ops/sec\n",
      "Initialization (trace) (epoch 11, progress 90.21%): UPDATE throughput 0.00 ops/sec, INSERT throughput 94385.92 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 94385.92 ops/sec\n",
      "Initialization (trace) (epoch 12, progress 98.45%): UPDATE throughput 0.00 ops/sec, INSERT throughput 96360.59 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 96360.59 ops/sec\n",
      "Initialization (trace): calculating overall performance metrics... (might take a while)\n",
      "Initialization (trace) overall: UPDATE throughput 0.00 ops/sec, INSERT throughput 95882.18 ops/sec, READ throughput 0.00 ops/sec, SCAN throughput 0.00 ops/sec, READ_MODIFY_WRITE throughput 0.00 ops/sec, total throughput 95882.18 ops/sec\n",
      "Initialization (trace) overall: UPDATE average latency 0.00 ns, UPDATE p99 latency 0.00 ns, INSERT average latency 5448.24 ns, INSERT p99 latency 9908.00 ns, READ average latency 0.00 ns, READ p99 latency 0.00 ns, SCAN average latency 0.00 ns, SCAN p99 latency 0.00 ns, READ_MODIFY_WRITE average latency 0.00 ns, READ_MODIFY_WRITE p99 latency 0.00 ns\n"
     ]
    }
   ],
   "source": [
    "def create_myycsb_yamls(init_filename: str, bench_filename: str, data_dir: str,\n",
    "                        init_workload_file: str, bench_workload_file: str,\n",
    "                        key_size: int, value_size: int, nr_entry: int) -> None:\n",
    "    # Create a YAML config file for My-YCSB initialization\n",
    "    config_init = {\n",
    "        \"database\": {\n",
    "            \"key_size\": key_size + 1,\n",
    "            \"value_size\": value_size + 1,\n",
    "            \"nr_entry\": nr_entry\n",
    "        },\n",
    "        \"workload\": {\n",
    "            \"nr_warmup_op\": 10000000,\n",
    "            \"warmup_runtime_seconds\": 240,\n",
    "            \"runtime_seconds\": 240,\n",
    "            \"nr_op\": 10000000,\n",
    "            \"nr_thread\": 8,\n",
    "            \"next_op_interval_ns\": 0,\n",
    "            \"operation_proportion\": {\n",
    "                \"read\": 0.5,\n",
    "                \"update\": 0.5,\n",
    "                \"insert\": 0,\n",
    "                \"scan\": 0,\n",
    "                \"read_modify_write\": 0\n",
    "            },\n",
    "            \"request_distribution\": \"trace\",\n",
    "            \"zipfian_constant\": 0.99,\n",
    "            \"trace_file\": init_workload_file,\n",
    "            \"trace_type\": \"twitter_init\",\n",
    "            \"scan_length\": 100\n",
    "        },\n",
    "        \"leveldb\": {\n",
    "            \"data_dir\": data_dir,\n",
    "            \"options_file\": \"/mydata/My-YCSB/rocksdb/config/rocksdb_rubble_16gb_config.ini\",\n",
    "            \"cache_size\": 100000000,\n",
    "            \"print_stats\": True\n",
    "        }\n",
    "    }\n",
    "\n",
    "    with open(init_filename, \"w\") as f:\n",
    "        yaml.dump(config_init, f)\n",
    "\n",
    "    config_bench = deepcopy(config_init)\n",
    "    config_bench[\"workload\"][\"trace_file\"] = bench_workload_file\n",
    "    config_bench[\"workload\"][\"trace_type\"] = \"twitter_bench\"\n",
    "\n",
    "    with open(bench_filename, \"w\") as f:\n",
    "        yaml.dump(config_bench, f)\n",
    "\n",
    "\n",
    "if create_data_dir:\n",
    "    if not os.path.exists(data_dir) or force:\n",
    "        if os.path.exists(data_dir):\n",
    "            print(\"Data directory already exists. Deleting.\")\n",
    "            run([\"rm\", \"-rf\", data_dir], check=True)\n",
    "        print(\"Creating data directory\")\n",
    "        os.makedirs(data_dir)\n",
    "        print(\"Creating My-YCSB init YAML file\")\n",
    "        create_myycsb_yamls(myycsb_init_yaml_file, myycsb_bench_yaml_file, data_dir,\n",
    "                            init_workload_file, bench_workload_file,\n",
    "                            key_size, value_size, num_unique_keys)\n",
    "        cmd = [\"/mydata/My-YCSB/build/init_leveldb\", myycsb_init_yaml_file]\n",
    "        print(\"Running My-YCSB init\")\n",
    "        run(cmd, check=True)\n",
    "    else:\n",
    "        print(\"Data directory already exists. Skipping creation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delete_trace_file:\n",
    "    print(\"Deleting trace file\")\n",
    "    os.remove(compressed_trace_file)\n",
    "    print(\"Deleted trace file\")\n",
    "else:\n",
    "    print(\"Trace file not deleted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
