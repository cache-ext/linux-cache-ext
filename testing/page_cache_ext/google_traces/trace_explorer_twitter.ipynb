{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import string\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from subprocess import run\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "# Previous Inputs:\n",
    "# - cluster_17, 0, 12M rows\n",
    "\n",
    "# Trace file (zstd compressed)\n",
    "cluster_name = \"cluster34\"\n",
    "# part = 0\n",
    "trace_file = \"/mydata/twitter_traces/%s\" % cluster_name\n",
    "compressed_trace_file = \"%s.sort.zst\" % trace_file\n",
    "num_rows_to_keep = 100 * 10**6\n",
    "\n",
    "init_workload_file = trace_file + \"_init.txt\"\n",
    "bench_workload_file = trace_file + \"_bench.txt\"\n",
    "\n",
    "# Trace analysis\n",
    "analyze_trace = True\n",
    "\n",
    "data_dir = \"data_twitter_%s\" % cluster_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace rows:  100000000\n"
     ]
    }
   ],
   "source": [
    "# Parse trace file into pandas dataframe\n",
    "# Line format is comma separated:\n",
    "# timestamp, key, key size, value size, client id, operation, ttl\n",
    "columns = [\"timestamp\", \"key\", \"key_size\", \"value_size\", \"client_id\", \"op_type\", \"ttl\"]\n",
    "trace = pd.read_csv(compressed_trace_file, names=columns, index_col=False, compression=\"zstd\", nrows=num_rows_to_keep)\n",
    "print(\"Trace rows: \", len(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "KiB = 2**10\n",
    "MiB = 2**20\n",
    "GiB = 2**30\n",
    "\n",
    "\n",
    "def format_number(num: int) -> str:\n",
    "    \"\"\"Format a number with commas.\"\"\"\n",
    "    return \"{:,}\".format(num)\n",
    "\n",
    "\n",
    "def format_bytes(num_bytes: int) -> str:\n",
    "    \"\"\"Format bytes in human-readable form.\"\"\"\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if num_bytes < 1024:\n",
    "            return f\"{num_bytes:.2f} {unit}\"\n",
    "        num_bytes /= 1024\n",
    "    return f\"{num_bytes:.2f} PB\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamps are ordered: True\n",
      "Keeping first 12,000,000 rows out of 125,111,121 total rows\n"
     ]
    }
   ],
   "source": [
    "# Check if timestamp column is ordered\n",
    "# Check if timestamps are monotonically increasing\n",
    "is_ordered = trace[\"timestamp\"].is_monotonic_increasing\n",
    "print(\"Timestamps are ordered:\", is_ordered)\n",
    "\n",
    "if not is_ordered:\n",
    "    # Sort the trace by timestamp\n",
    "    print(\"Sorting trace by timestamp...\")\n",
    "    trace.sort_values(\"timestamp\", inplace=True)\n",
    "    # Verify sorting worked\n",
    "    is_ordered = trace[\"timestamp\"].is_monotonic_increasing\n",
    "    # Overwrite trace file\n",
    "    print(\"Saving sorted trace...\")\n",
    "    trace.to_csv(trace_file, index=False, header=False)\n",
    "    print(\"Sorted trace saved\")\n",
    "\n",
    "# Keep only first num_rows_to_keep rows\n",
    "if len(trace) > num_rows_to_keep:\n",
    "    print(f\"Keeping first {num_rows_to_keep:,} rows out of {len(trace):,} total rows\")\n",
    "    trace = trace.head(num_rows_to_keep)\n",
    "else:\n",
    "    print(f\"Trace has {len(trace):,} rows, no truncation needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value size counts:\n",
      "1         4544025\n",
      "109       2906983\n",
      "0          656976\n",
      "102        585666\n",
      "39         422552\n",
      "           ...   \n",
      "12866           1\n",
      "25454           1\n",
      "11674           1\n",
      "152946          1\n",
      "21969           1\n",
      "Name: value_size, Length: 27848, dtype: int64\n",
      "Key size counts:\n",
      "42    1284899\n",
      "14    1147953\n",
      "15     963136\n",
      "43     949503\n",
      "26     900665\n",
      "33     828715\n",
      "24     811886\n",
      "23     766890\n",
      "13     561270\n",
      "34     556306\n",
      "32     467932\n",
      "41     446652\n",
      "27     441010\n",
      "35     436206\n",
      "36     397251\n",
      "25     395657\n",
      "44     208764\n",
      "31     110063\n",
      "22      88919\n",
      "29      59064\n",
      "20      36094\n",
      "19      32683\n",
      "28      30634\n",
      "12      28763\n",
      "16      23648\n",
      "21      13162\n",
      "18       8561\n",
      "30       1441\n",
      "11       1308\n",
      "9         791\n",
      "17        113\n",
      "10         55\n",
      "8           6\n",
      "Name: key_size, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print a list of value sizes and their counts in decreasing order\n",
    "value_size_counts = trace[\"value_size\"].value_counts()\n",
    "print(\"Value size counts:\")\n",
    "print(value_size_counts)\n",
    "\n",
    "key_size_counts = trace[\"key_size\"].value_counts()\n",
    "print(\"Key size counts:\")\n",
    "print(key_size_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean value size:  512\n",
      "Median value size:  1\n",
      "Mean key size:  32\n",
      "Median key size:  33\n",
      "Key size:  32\n",
      "Value size:  512\n"
     ]
    }
   ],
   "source": [
    "# Print mean and median value size. Make sure keys first.\n",
    "# Calculate mean and median value sizes\n",
    "mean_value_size = trace.groupby(\"key\")[\"value_size\"].mean().mean()\n",
    "median_value_size = trace.groupby(\"key\")[\"value_size\"].median().median()\n",
    "\n",
    "print(\"Mean value size: \", int(mean_value_size))\n",
    "print(\"Median value size: \", int(median_value_size))\n",
    "\n",
    "# Convert key_size to numeric, dropping any non-numeric values\n",
    "temp_trace = trace.copy()\n",
    "temp_trace[\"key_size\"] = pd.to_numeric(temp_trace[\"key_size\"], errors=\"coerce\")\n",
    "# Keep only rows where key_size is not null (was successfully converted to numeric)\n",
    "temp_trace = temp_trace[temp_trace[\"key_size\"].notna()]\n",
    "\n",
    "\n",
    "# Calculate mean and median key sizes\n",
    "mean_key_size = temp_trace.groupby(\"key\")[\"key_size\"].mean().mean()\n",
    "median_key_size = temp_trace.groupby(\"key\")[\"key_size\"].median().median()\n",
    "\n",
    "print(\"Mean key size: \", int(mean_key_size))\n",
    "print(\"Median key size: \", int(median_key_size))\n",
    "\n",
    "key_size = int(mean_key_size)\n",
    "value_size = int(mean_value_size)\n",
    "\n",
    "print(\"Key size: \", key_size)\n",
    "print(\"Value size: \", value_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys needed for 100GB dataset:  309,588,024\n"
     ]
    }
   ],
   "source": [
    "# Calculate how many keys are needed for a ~100GB dataset\n",
    "target_size = 110 * GiB\n",
    "num_keys = target_size / (mean_key_size + mean_value_size)\n",
    "print(\"Number of keys needed for 100GB dataset: \", format_number(int(num_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How big is the working set?\n",
    "wss = trace[\"key\"].nunique() * (key_size + value_size)\n",
    "print(\"Working set size: \", format_bytes(wss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in trace: 50000000\n",
      "Read operations: 46912736\n",
      "Write operations: 3087264\n",
      "Read proportion: 93.83%\n",
      "Working set size:  3.49 GB\n"
     ]
    }
   ],
   "source": [
    "# How many lines is the trace?\n",
    "print(\"Number of lines in trace: %d\" % len(trace))\n",
    "\n",
    "# What is the proportion of read and write operations?\n",
    "read_ops = len(trace[trace[\"op_type\"] == \"get\"])\n",
    "write_ops = len(trace[trace[\"op_type\"] == \"set\"])\n",
    "print(\"Read operations: %d\" % read_ops)\n",
    "print(\"Write operations: %d\" % write_ops)\n",
    "print(\"Read proportion: %.2f%%\" % (100 * read_ops / (read_ops + write_ops)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 306,201,418 remaining keys\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210390/2680972809.py:40: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  keys = list(tqdm(pool.imap(random_keys_single_arg, args), total=len(args), desc=\"Generating keys\"))\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.012936115264892578,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating keys",
       "rate": null,
       "total": 30621,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f1cc900883b492aae73847e04a16783",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating keys:   0%|          | 0/30621 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing keys to file\n"
     ]
    }
   ],
   "source": [
    "# Prefix pad keys with zeros to make them the same length.\n",
    "# Alternatively, trim keys to a fixed length.\n",
    "def pad_key(key: str, target_size: int) -> str:\n",
    "    # If key is longer than target size, truncate it\n",
    "    if len(key) > target_size:\n",
    "        return key[:target_size]\n",
    "    # If key is shorter than target size, pad with zeros\n",
    "    return key.zfill(target_size)\n",
    "\n",
    "# Generate a random alphanumeric key of a given size\n",
    "def random_key(key_size: int) -> str:\n",
    "    # Generate a random string of digits and letters\n",
    "    chars = string.ascii_letters + string.digits\n",
    "    return \"\".join(random.choice(chars) for _ in range(key_size))\n",
    "\n",
    "\n",
    "def random_keys(num_keys: int, key_size: int) -> List[str]:\n",
    "    return [random_key(key_size) for _ in range(num_keys)]\n",
    "\n",
    "\n",
    "def random_keys_single_arg(args):\n",
    "    return random_keys(*args)\n",
    "\n",
    "\n",
    "def random_keys_parallel(num_keys: int, key_size: int, num_processes: int) -> List[str]:\n",
    "    # Split the number of keys into chunks of 10k keys\n",
    "    chunk_size = 10000\n",
    "    num_chunks = num_keys // chunk_size\n",
    "    remaining_keys = num_keys % chunk_size\n",
    "\n",
    "    # Create list of tuples with (num_keys, key_size) for each process\n",
    "    args = [(chunk_size, key_size)] * num_chunks\n",
    "\n",
    "    # Add remaining keys as final chunk if needed\n",
    "    if remaining_keys > 0:\n",
    "        args.append((remaining_keys, key_size))\n",
    "\n",
    "    # Use multiprocessing pool with progress bar\n",
    "    with mp.Pool(num_processes) as pool:\n",
    "        keys = list(tqdm(pool.imap(random_keys_single_arg, args), total=len(args), desc=\"Generating keys\"))\n",
    "\n",
    "    # Flatten the list of lists\n",
    "    return [key for sublist in keys for key in sublist]\n",
    "\n",
    "def generate_init_workload(trace: pd.DataFrame, key_size: int, num_keys: int, filename: str) -> None:\n",
    "    # Construct the init workload file\n",
    "    # Each line is a key\n",
    "    # All keys and values are the same size\n",
    "    trace_keys = list(trace[\"key\"].unique())\n",
    "    trace_keys = [pad_key(key, key_size) for key in trace_keys]\n",
    "\n",
    "    num_remaining_keys = int(num_keys) - len(trace_keys)\n",
    "    print(\"Generating %s remaining keys\" % format_number(num_remaining_keys))\n",
    "    remaining_keys = random_keys_parallel(num_remaining_keys, key_size, mp.cpu_count())\n",
    "\n",
    "    # Combine trace keys and remaining keys\n",
    "    all_keys = trace_keys + remaining_keys\n",
    "\n",
    "    # Permuate the keys\n",
    "    random.shuffle(all_keys)\n",
    "\n",
    "    # Write keys to file\n",
    "    print(\"Writing keys to file\")\n",
    "    with open(filename, \"w\") as f:\n",
    "        for key in all_keys:\n",
    "            f.write(key + \"\\n\")\n",
    "\n",
    "generate_init_workload(trace, key_size, num_keys, init_workload_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.20 GB'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get    11342618\n",
       "set      657382\n",
       "Name: op_type, dtype: int64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace[\"op_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating bench workload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_210390/1052782408.py:12: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for _, row in tqdm(trace.iterrows(), total=len(trace), desc=\"Writing operations\"):\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004571199417114258,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Writing operations",
       "rate": null,
       "total": 12000000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "352da335c1b0493482ccaacabec3d9d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Writing operations:   0%|          | 0/12000000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done generating bench workload\n"
     ]
    }
   ],
   "source": [
    "# Now generate the bench workload file\n",
    "# Each line is an operation followed by a key (space separated)\n",
    "# Operations are get or insert\n",
    "\n",
    "def generate_bench_workload(trace: pd.DataFrame, key_size: int, filename: str) -> None:\n",
    "    # Construct the bench workload file\n",
    "    # Each line is an operation followed by a key (space separated)\n",
    "    print(\"Generating bench workload\")\n",
    "\n",
    "    with open(filename, \"w\") as f:\n",
    "        # Iterate through trace rows with progress bar\n",
    "        for _, row in tqdm(trace.iterrows(), total=len(trace), desc=\"Writing operations\"):\n",
    "            key = pad_key(str(row[\"key\"]), key_size)\n",
    "            # Write operation and key\n",
    "            if row[\"op_type\"] == \"get\":\n",
    "                f.write(\"get \" + key + \"\\n\")\n",
    "            elif row[\"op_type\"] == \"cas\":\n",
    "                f.write(\"update \" + key + \"\\n\")\n",
    "            else:\n",
    "                f.write(\"insert \" + key + \"\\n\")\n",
    "\n",
    "    print(\"Done generating bench workload\")\n",
    "\n",
    "\n",
    "generate_bench_workload(trace, key_size, bench_workload_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running trace analysis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dat: cluster34.000\n",
      "number of requests: 116734382, number of objects: 8697706\n",
      "number of req GiB: 22.5188, number of obj GiB: 7.3966\n",
      "compulsory miss ratio (req/byte): 0.0745/0.3285\n",
      "object size weighted by req/obj: 207/913\n",
      "frequency mean: 13.4213\n",
      "time span: 37098(0.4294 day)\n",
      "write: 0(0), overwrite: 0(0), del:0(0)\n",
      "request rate min 1576.1800 req/s, max 3952.9600 req/s, window 300s\n",
      "object rate min 955.8100 obj/s, max 2303.3500 obj/s, window 300s\n",
      "popularity: Zipf linear fitting slope=1.0368, intercept=-1.0000, R2=-1.0000\n",
      "X-hit (number of obj accessed X times): 1710366(0.1966), 2233810(0.2568), 1132913(0.1303), 611823(0.0703), 398030(0.0458), 318951(0.0367), 236664(0.0272), 177397(0.0204), \n",
      "freq (fraction) of the most popular obj: 4340172(0.0372), 1177272(0.0101), 1166011(0.0100), 972589(0.0083), 936991(0.0080), 918408(0.0079), 770925(0.0066), 709380(0.0061), \n",
      "\n"
     ]
    }
   ],
   "source": [
    "libcachesim_dir = \"/mydata/libCacheSim\"\n",
    "trace_analyzer = os.path.join(libcachesim_dir, \"_build/bin/traceAnalyzer\")\n",
    "\n",
    "if not os.path.exists(trace_analyzer):\n",
    "    print(\"libCacheSim traceAnalyzer not found. Skipping trace analysis.\")\n",
    "elif analyze_trace:\n",
    "    cmd = [trace_analyzer, trace_file, \"csv\", \"--common\", \"-t\", \"time-col=1, obj-id-col=2, obj-size-col=4, delimiter=,, has-header=false\"]\n",
    "    print(\"Running trace analysis\")\n",
    "    run(cmd, check=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
