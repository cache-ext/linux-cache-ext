{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from subprocess import run\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "cluster_name = \"cluster1_16TB\"\n",
    "date = \"20240119\"\n",
    "start_idx = 0\n",
    "num_traces_to_include = 100\n",
    "url_template = \"https://storage.googleapis.com/thesios-io-traces/%s/%s/%s\"\n",
    "initialize_data_dir = False\n",
    "remove_zero_size_ops = True\n",
    "data_file_from_idx = lambda idx: \"data-00%s-of-00100\" % str(idx).zfill(3)\n",
    "app_to_keep = \"bigtable\"\n",
    "num_ops_to_keep = 700000\n",
    "trace_folder = \"traces\"\n",
    "keep_only = \"both\" # \"reads\", \"writes\", \"both\"\n",
    "\n",
    "# Computed\n",
    "def format_app_to_keep(s: str) -> str:\n",
    "    if len(s) < 12:\n",
    "        return s\n",
    "    return s[:8]\n",
    "\n",
    "assert start_idx >= 0 and start_idx <= 100, \"Invalid start_idx: %d\" % start_idx\n",
    "end_idx = start_idx + num_traces_to_include - 1\n",
    "assert end_idx >= 0 and end_idx <= 100, \"Invalid end_idx: %d\" % end_idx\n",
    "output_trace_file = \"trace_%s_%s\" % (cluster_name, date)\n",
    "if app_to_keep:\n",
    "    output_trace_file += \"_app_%s\" % format_app_to_keep(app_to_keep)\n",
    "if keep_only != \"both\":\n",
    "    output_trace_file += \"_%s\" % keep_only\n",
    "output_trace_file += \".txt\"\n",
    "data_dir = \"data_%s_%s\" % (cluster_name, date)\n",
    "if app_to_keep:\n",
    "    data_dir += \"_app_%s\" % format_app_to_keep(app_to_keep)\n",
    "if keep_only != \"both\":\n",
    "    data_dir += \"_%s\" % keep_only\n",
    "data_dir += \"_temp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(trace_folder):\n",
    "    os.makedirs(trace_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def download_to_file_single_arg(args):\n",
    "    return download_to_file(*args)\n",
    "\n",
    "def download_to_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def format_bytes(bytes: int) -> str:\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if bytes < 1024:\n",
    "            return f\"{bytes:.2f} {unit}\"\n",
    "        bytes /= 1024\n",
    "    return f\"{bytes:.2f} PB\"\n",
    "\n",
    "def process_trace(trace) -> Dict[str, Dict[str, int]]:\n",
    "    # We want to process the trace, operation by operation.\n",
    "    # Let's simplify: assume everything pre-exists, and we only need to keep track of the last read/write offset for each file.\n",
    "\n",
    "    # We will keep track of the last read offset for each file.\n",
    "    # Initialize dictionaries to store file metadata\n",
    "    file_metadata = {}\n",
    "\n",
    "    for _, row in trace.iterrows():\n",
    "        filename = row[\"filename\"]\n",
    "        offset = row[\"file_offset\"]\n",
    "        io_size = row[\"request_io_size_bytes\"]\n",
    "        operation = row[\"op_type\"]\n",
    "\n",
    "        if filename not in file_metadata:\n",
    "            file_metadata[filename] = {\n",
    "                \"max_read_offset\": 0,\n",
    "                \"max_write_offset\": 0,\n",
    "                \"max_initialized_offset\": 0,\n",
    "            }\n",
    "\n",
    "        metadata = file_metadata[filename]\n",
    "\n",
    "        if operation == \"READ\":\n",
    "            metadata[\"max_initialized_offset\"] = max(metadata[\"max_initialized_offset\"], offset + io_size)\n",
    "            metadata[\"max_read_offset\"] = max(metadata[\"max_read_offset\"], offset + io_size)\n",
    "        elif operation == \"WRITE\":\n",
    "            metadata[\"max_initialized_offset\"] = max(metadata[\"max_initialized_offset\"], offset + io_size)\n",
    "            metadata[\"max_write_offset\"] = max(metadata[\"max_write_offset\"], offset + io_size)\n",
    "\n",
    "    return file_metadata\n",
    "\n",
    "def print_file_metadata_stats(file_metadata) -> Tuple[int, int, int]:\n",
    "    total_size_initialized = sum([metadata[\"max_initialized_offset\"] for metadata in file_metadata.values()])\n",
    "    total_size_read = sum([metadata[\"max_read_offset\"] for metadata in file_metadata.values()])\n",
    "    total_size_written = sum([metadata[\"max_write_offset\"] for metadata in file_metadata.values()])\n",
    "\n",
    "    print(f\"Total size initialized: {format_bytes(total_size_initialized)}\")\n",
    "    print(f\"Total size read: {format_bytes(total_size_read)}\")\n",
    "    print(f\"Total size written: {format_bytes(total_size_written)}\")\n",
    "    return total_size_initialized, total_size_read, total_size_written\n",
    "\n",
    "# Initialize a new directory and write the files needed to run the trace\n",
    "def initialize_file(file_path: str, size: int):\n",
    "    \"\"\"Initialize a file with pseudo-random data.\"\"\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        # Write data in chunks to avoid excessive memory usage\n",
    "        chunk_size = 16 * 1024 * 1024  # 16 MB chunks\n",
    "        remaining_size = size\n",
    "\n",
    "        while remaining_size > 0:\n",
    "            write_size = min(chunk_size, remaining_size)\n",
    "            data = np.random.bytes(write_size)\n",
    "            f.write(data)\n",
    "            remaining_size -= write_size\n",
    "    # print(f\"Initialized file: {file_path} with size: {format_bytes(size)}\")\n",
    "\n",
    "def initialize_file_single_arg(args):\n",
    "    return initialize_file(*args)\n",
    "\n",
    "def create_trace_data_dir(data_dir: str, file_metadata: Dict[str, Dict[str, int]]):\n",
    "    print(\"Initializing data directory: %s\" % data_dir)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    else:\n",
    "        print(\"Data directory already exists, not doing anything.\")\n",
    "        return\n",
    "\n",
    "    # Process files in parallel\n",
    "    total_files = len(file_metadata)\n",
    "    print(f\"Initializing {total_files} files...\")\n",
    "\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        tasks = []\n",
    "        for filename, metadata in file_metadata.items():\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            size = metadata[\"max_initialized_offset\"]\n",
    "            tasks.append((file_path, size))\n",
    "\n",
    "        # Use multiprocessing to parallelize file initialization\n",
    "        for _ in tqdm(pool.imap(initialize_file_single_arg, tasks), total=total_files):\n",
    "            pass\n",
    "\n",
    "    print(\"File initialization complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_files = []\n",
    "download_tasks = []\n",
    "for idx in range(start_idx, end_idx + 1):\n",
    "    data_file = data_file_from_idx(idx)\n",
    "    local_file = \"%s_%s_%s.csv\" % (cluster_name, date, data_file)\n",
    "    local_file = os.path.join(trace_folder, local_file)\n",
    "    download_url = url_template % (cluster_name, date, data_file)\n",
    "    local_files.append(local_file)\n",
    "\n",
    "    if not os.path.exists(local_file):\n",
    "        print(\"Downloading %s to %s\" % (download_url, local_file))\n",
    "        download_tasks.append((download_url, local_file))\n",
    "\n",
    "if len(download_tasks) > 0:\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        for _ in tqdm(pool.imap(download_to_file_single_arg, download_tasks), total=len(download_tasks)):\n",
    "            pass\n",
    "\n",
    "# Combine all the traces into a single file\n",
    "trace_dfs = [ pd.read_csv(local_file) for local_file in local_files ]\n",
    "combined_trace = pd.concat(trace_dfs, ignore_index=True)\n",
    "trace = combined_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>eac0f0c95eb5e8381ce9d2194fe38447a7d4f4a67a8ce6...</td>\n",
       "      <td>6582272</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705174034</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94208</td>\n",
       "      <td>98304</td>\n",
       "      <td>94208</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a54a6612f9ca88d16f8b0f7f2628fafac909388ddab3ee...</td>\n",
       "      <td>4332227</td>\n",
       "      <td>spanner</td>\n",
       "      <td>1705651081</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>LATENCY_SENSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>37975</td>\n",
       "      <td>37975</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>c7cf9b09cbcc7a9dea16054e3b723795478b6ced944e5e...</td>\n",
       "      <td>6519893</td>\n",
       "      <td>91a5869bf23221afed516b34207d3e3514152f38c4c97a...</td>\n",
       "      <td>1705651194</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>c7cf9b09cbcc7a9dea16054e3b723795478b6ced944e5e...</td>\n",
       "      <td>6519893</td>\n",
       "      <td>91a5869bf23221afed516b34207d3e3514152f38c4c97a...</td>\n",
       "      <td>1705651194</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>475861</td>\n",
       "      <td>475861</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6c70c7107b38a4a7fa4fad3564dc0010c34ee1446eb48b...</td>\n",
       "      <td>4878336</td>\n",
       "      <td>spanner</td>\n",
       "      <td>1705046465</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>364544</td>\n",
       "      <td>360448</td>\n",
       "      <td>364544</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.004412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  file_offset  \\\n",
       "0  eac0f0c95eb5e8381ce9d2194fe38447a7d4f4a67a8ce6...      6582272   \n",
       "1  a54a6612f9ca88d16f8b0f7f2628fafac909388ddab3ee...      4332227   \n",
       "2  c7cf9b09cbcc7a9dea16054e3b723795478b6ced944e5e...      6519893   \n",
       "3  c7cf9b09cbcc7a9dea16054e3b723795478b6ced944e5e...      6519893   \n",
       "4  6c70c7107b38a4a7fa4fad3564dc0010c34ee1446eb48b...      4878336   \n",
       "\n",
       "                                         application      c_time io_zone  \\\n",
       "0                                           bigtable  1705174034    WARM   \n",
       "1                                            spanner  1705651081    WARM   \n",
       "2  91a5869bf23221afed516b34207d3e3514152f38c4c97a...  1705651194    WARM   \n",
       "3  91a5869bf23221afed516b34207d3e3514152f38c4c97a...  1705651194    WARM   \n",
       "4                                            spanner  1705046465    WARM   \n",
       "\n",
       "  redundancy_type op_type        service_class  from_flash_cache  cache_hit  \\\n",
       "0   ERASURE_CODED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "1      REPLICATED   WRITE    LATENCY_SENSITIVE                 0         -1   \n",
       "2      REPLICATED   WRITE  THROUGHPUT_ORIENTED                 0         -1   \n",
       "3      REPLICATED   WRITE  THROUGHPUT_ORIENTED                 0         -1   \n",
       "4   ERASURE_CODED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "\n",
       "   request_io_size_bytes  disk_io_size_bytes  response_io_size_bytes  \\\n",
       "0                  94208               98304                   94208   \n",
       "1                  37975               37975                       0   \n",
       "2                      0                   0                       0   \n",
       "3                 475861              475861                       0   \n",
       "4                 364544              360448                  364544   \n",
       "\n",
       "     start_time  disk_time  simulated_disk_start_time  simulated_latency  \n",
       "0  1.705651e+09   0.005128               1.705651e+09           0.005128  \n",
       "1  1.705651e+09   0.000000               0.000000e+00           0.000052  \n",
       "2  1.705651e+09   0.000000               0.000000e+00           0.000053  \n",
       "3  1.705651e+09   0.000000               0.000000e+00           0.000374  \n",
       "4  1.705651e+09   0.001878               1.705651e+09           0.004412  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique applications: 405\n",
      "Number of operations per application:\n",
      "application\n",
      "eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d336d87d7adda2a74b10    4829717\n",
      "spanner                                                             4780942\n",
      "bigtable                                                            1401456\n",
      "ad263109a23dfb7acc3d450adb38df34455e00e2982ddd99436119eace25e970     421601\n",
      "c1362825bb92f7917efc82b51ebad7d906bc575c1df81dc1f4f5f9fefff70773     258239\n",
      "                                                                     ...   \n",
      "4187ce1b2ce7527abe8ee8423f2d93402a29cff77bf07227af533f0275dd7f53          1\n",
      "a2d737b427ca1079e35f9620bd0734ecef1dec0ef2794bac4ba59a000e40c46b          1\n",
      "63cb71b70abf663c400d550b4020e77adf89af691e0fddf11b2654e411eb5748          1\n",
      "e3c382fe554e0dbf9533148de6a9f4ef7d408091a74e120c3947fa577b5de9d9          1\n",
      "611f2348c6b5d7d6d9c7e74a2bafe9c1d7c51f230d4e6165d66a9c4508923f07          1\n",
      "Length: 405, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print unique applications\n",
    "print(\"Number of unique applications: %d\" % len(trace[\"application\"].unique()))\n",
    "\n",
    "# Print number of operations per application\n",
    "operations_per_application = trace.groupby(\"application\").size().sort_values(ascending=False)\n",
    "print(\"Number of operations per application:\")\n",
    "print(operations_per_application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_zero_size_ops:\n",
    "    trace = trace[trace[\"request_io_size_bytes\"] > 0].reset_index()\n",
    "if app_to_keep and len(app_to_keep) > 0:\n",
    "    trace = trace[trace[\"application\"] == app_to_keep].head(num_ops_to_keep).reset_index(drop=True).copy()\n",
    "\n",
    "if keep_only == \"reads\":\n",
    "    trace = trace[trace[\"op_type\"] == \"READ\"].reset_index(drop=True).copy()\n",
    "elif keep_only == \"writes\":\n",
    "    trace = trace[trace[\"op_type\"] == \"WRITE\"].reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in trace: 700000\n",
      "Read operations: 298471\n",
      "Write operations: 401529\n",
      "Read proportion: 42.64%\n",
      "Read bytes: 88.06 GB\n",
      "Write bytes: 40.54 GB\n",
      "Total bytes: 128.60 GB\n"
     ]
    }
   ],
   "source": [
    "# How many lines is the trace?\n",
    "print(\"Number of lines in trace: %d\" % len(trace))\n",
    "\n",
    "# What is the proportion of read and write operations?\n",
    "read_ops = len(trace[trace[\"op_type\"] == \"READ\"])\n",
    "write_ops = len(trace[trace[\"op_type\"] == \"WRITE\"])\n",
    "print(\"Read operations: %d\" % read_ops)\n",
    "print(\"Write operations: %d\" % write_ops)\n",
    "print(\"Read proportion: %.2f%%\" % (100 * read_ops / (read_ops + write_ops)))\n",
    "\n",
    "read_bytes = trace[trace[\"op_type\"] == \"READ\"][\"request_io_size_bytes\"].sum()\n",
    "write_bytes = trace[trace[\"op_type\"] == \"WRITE\"][\"request_io_size_bytes\"].sum()\n",
    "total_bytes = read_bytes + write_bytes\n",
    "print(\"Read bytes: %s\" % format_bytes(read_bytes))\n",
    "print(\"Write bytes: %s\" % format_bytes(write_bytes))\n",
    "print(\"Total bytes: %s\" % format_bytes(total_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique filenames: 34087\n",
      "\n",
      "Unique filenames:\n",
      "['eac0f0c95eb5e8381ce9d2194fe38447a7d4f4a67a8ce658c5dd1d1ba52b02d9'\n",
      " '4f7d915d7e89c11316c83373721f324a500858d7799da0c54dfe6a79229f77e2'\n",
      " '7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9f470bd5ae5396d1357' ...\n",
      " '8f6dfce9900bb087d1f1a10280ba1c19caa8a242e4957d65aef70107d5fda198'\n",
      " 'a27cd439f2edf5f71a65a334b70ecc6c24917aacd86ef8100b60a5993b8ef830'\n",
      " '04baa03f476619b6bccb034a0844202fe2613b3d944afe22a36801f5627be92e']\n"
     ]
    }
   ],
   "source": [
    "num_unique_filenames = trace[\"filename\"].nunique()\n",
    "print(f\"Number of unique filenames: {num_unique_filenames}\")\n",
    "print(\"\\nUnique filenames:\")\n",
    "print(trace[\"filename\"].unique())\n",
    "if num_unique_filenames > 100000:\n",
    "    raise Exception(\"Too many unique filenames: %d\" % num_unique_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size initialized: 122.47 GB\n",
      "Total size read: 108.13 GB\n",
      "Total size written: 40.54 GB\n"
     ]
    }
   ],
   "source": [
    "file_metadata = process_trace(trace)\n",
    "total_size_initialized, total_size_read, total_size_written = print_file_metadata_stats(file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "GiB = 2 ** 30\n",
    "if total_size_initialized > 370 * GiB:\n",
    "    raise Exception(\"Too much data initialized: %s\" % format_bytes(total_size_initialized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "if initialize_data_dir:\n",
    "    create_trace_data_dir(data_dir, file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>eac0f0c95eb5e8381ce9d2194fe38447a7d4f4a67a8ce6...</td>\n",
       "      <td>6582272</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705174034</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94208</td>\n",
       "      <td>98304</td>\n",
       "      <td>94208</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51</td>\n",
       "      <td>4f7d915d7e89c11316c83373721f324a500858d7799da0...</td>\n",
       "      <td>0</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705651200</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>LATENCY_SENSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>215446</td>\n",
       "      <td>215446</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...</td>\n",
       "      <td>6291456</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705046784</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>184320</td>\n",
       "      <td>1048576</td>\n",
       "      <td>184320</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.037774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...</td>\n",
       "      <td>6660096</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705046784</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>77824</td>\n",
       "      <td>0</td>\n",
       "      <td>77824</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...</td>\n",
       "      <td>7077888</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705046784</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>262144</td>\n",
       "      <td>0</td>\n",
       "      <td>262144</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           filename  file_offset  \\\n",
       "0      0  eac0f0c95eb5e8381ce9d2194fe38447a7d4f4a67a8ce6...      6582272   \n",
       "1     51  4f7d915d7e89c11316c83373721f324a500858d7799da0...            0   \n",
       "2     56  7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...      6291456   \n",
       "3     60  7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...      6660096   \n",
       "4     61  7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...      7077888   \n",
       "\n",
       "  application      c_time io_zone redundancy_type op_type  \\\n",
       "0    bigtable  1705174034    WARM   ERASURE_CODED    READ   \n",
       "1    bigtable  1705651200    WARM      REPLICATED   WRITE   \n",
       "2    bigtable  1705046784    WARM   ERASURE_CODED    READ   \n",
       "3    bigtable  1705046784    WARM   ERASURE_CODED    READ   \n",
       "4    bigtable  1705046784    WARM   ERASURE_CODED    READ   \n",
       "\n",
       "         service_class  from_flash_cache  cache_hit  request_io_size_bytes  \\\n",
       "0  THROUGHPUT_ORIENTED                 0          0                  94208   \n",
       "1    LATENCY_SENSITIVE                 0         -1                 215446   \n",
       "2  THROUGHPUT_ORIENTED                 0          0                 184320   \n",
       "3  THROUGHPUT_ORIENTED                 0          1                  77824   \n",
       "4  THROUGHPUT_ORIENTED                 1          1                 262144   \n",
       "\n",
       "   disk_io_size_bytes  response_io_size_bytes    start_time  disk_time  \\\n",
       "0               98304                   94208  1.705651e+09   0.005128   \n",
       "1              215446                       0  1.705651e+09   0.000000   \n",
       "2             1048576                  184320  1.705651e+09   0.004203   \n",
       "3                   0                   77824  1.705651e+09   0.000000   \n",
       "4                   0                  262144  1.705651e+09   0.000000   \n",
       "\n",
       "   simulated_disk_start_time  simulated_latency  \n",
       "0               1.705651e+09           0.005128  \n",
       "1               0.000000e+00           0.000113  \n",
       "2               1.705651e+09           0.037774  \n",
       "3               0.000000e+00           0.000068  \n",
       "4               0.000000e+00           0.000077  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace file already exists: trace_cluster1_16TB_20240119_app_bigtable.txt\n",
      "Not overwriting.\n"
     ]
    }
   ],
   "source": [
    "# Format the trace file to be used in the benchmark client\n",
    "# The expected format is:\n",
    "# <op_type> <filename>:<offset>:<size>\n",
    "# Example:\n",
    "# READ file1:0:4096\n",
    "# WRITE file2:0:4096\n",
    "\n",
    "def convert_trace_to_bench_client_format(trace: pd.DataFrame, output_file: str):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _, row in trace.iterrows():\n",
    "            filename = row[\"filename\"]\n",
    "            offset = row[\"file_offset\"]\n",
    "            io_size = row[\"request_io_size_bytes\"]\n",
    "            operation = row[\"op_type\"]\n",
    "\n",
    "            f.write(f\"{operation} {filename}:{offset}:{io_size}\\n\")\n",
    "\n",
    "if not os.path.exists(output_trace_file):\n",
    "    print(\"Converting trace to benchmark client format...\")\n",
    "    convert_trace_to_bench_client_format(trace, output_trace_file)\n",
    "    print(\"Trace file converted to benchmark client format: %s\" % output_trace_file)\n",
    "else:\n",
    "    print(\"Trace file already exists: %s\" % output_trace_file)\n",
    "    print(\"Not overwriting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libcachesim trace file already exists: trace_cluster1_16TB_20240119_app_bigtable_libcachesim.csv\n",
      "Not overwriting.\n",
      "libcachesim bninary trace file already exists: trace_cluster1_16TB_20240119_app_bigtable_libcachesim.csv.oracleGeneral\n"
     ]
    }
   ],
   "source": [
    "PAGE_SIZE = 4096\n",
    "\n",
    "def convert_trace_to_libcachesim_format(trace: pd.DataFrame, output_file: str):\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"time,obj_id,obj_size\\n\")\n",
    "        for _, row in trace.iterrows():\n",
    "            filename = row[\"filename\"]\n",
    "            offset = row[\"file_offset\"]\n",
    "            timestamp = row[\"start_time\"]\n",
    "            io_size = row[\"request_io_size_bytes\"]\n",
    "            start_page_num = offset // PAGE_SIZE\n",
    "            end_page_num = math.ceil((offset + io_size) / PAGE_SIZE)\n",
    "            for page_num in range(start_page_num, end_page_num):\n",
    "                offset = page_num * PAGE_SIZE\n",
    "                io_size = PAGE_SIZE\n",
    "                obj_id = f\"{filename}:{offset}\"\n",
    "                f.write(f\"{timestamp},{obj_id},{io_size}\\n\")\n",
    "\n",
    "prefix = output_trace_file\n",
    "if prefix.endswith(\".txt\"):\n",
    "    prefix = prefix[:-4]\n",
    "output_trace_file_libcachesim = prefix + \"_libcachesim.csv\"\n",
    "if not os.path.exists(output_trace_file_libcachesim):\n",
    "    print(\"Converting trace to libcachesim format...\")\n",
    "    convert_trace_to_libcachesim_format(trace, output_trace_file_libcachesim)\n",
    "    print(\"trace file converted to libcachesim format: %s\" % output_trace_file_libcachesim)\n",
    "else:\n",
    "    print(\"libcachesim trace file already exists: %s\" % output_trace_file_libcachesim)\n",
    "    print(\"Not overwriting.\")\n",
    "\n",
    "\n",
    "output_trace_file_libcachesim_binary = output_trace_file_libcachesim + \".oracleGeneral\"\n",
    "if not os.path.exists(output_trace_file_libcachesim_binary):\n",
    "    print(\"Converting trace to libcachesim binary format...\")\n",
    "    trace_conv_bin = \"/mydata/libCacheSim/_build/bin/traceConv\"\n",
    "    if not os.path.exists(trace_conv_bin):\n",
    "        raise Exception(\"traceConv binary does not exist: %s\" % trace_conv_bin)\n",
    "    csv_opts = \"time-col=1, obj-id-col=2, obj-size-col=3, delimiter=,, has-header=true\"\n",
    "    cmd = [trace_conv_bin, output_trace_file_libcachesim, \"csv\", \"-t\", csv_opts]\n",
    "    run(cmd)\n",
    "    print(\"trace file converted to libcachesim binary format: %s\" % output_trace_file_libcachesim_binary)\n",
    "else:\n",
    "    print(\"libcachesim bninary trace file already exists: %s\" % output_trace_file_libcachesim_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where request_io_size_bytes != disk_io_size_bytes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>eac0f0c95eb5e8381ce9d2194fe38447a7d4f4a67a8ce6...</td>\n",
       "      <td>6582272</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705174034</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>94208</td>\n",
       "      <td>98304</td>\n",
       "      <td>94208</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.005128</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.005128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56</td>\n",
       "      <td>7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...</td>\n",
       "      <td>6291456</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705046784</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>184320</td>\n",
       "      <td>1048576</td>\n",
       "      <td>184320</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.037774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...</td>\n",
       "      <td>6660096</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705046784</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>77824</td>\n",
       "      <td>0</td>\n",
       "      <td>77824</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>61</td>\n",
       "      <td>7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...</td>\n",
       "      <td>7077888</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705046784</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>262144</td>\n",
       "      <td>0</td>\n",
       "      <td>262144</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>62</td>\n",
       "      <td>7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...</td>\n",
       "      <td>6553600</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705046784</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>262144</td>\n",
       "      <td>0</td>\n",
       "      <td>262144</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000054</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           filename  file_offset  \\\n",
       "0      0  eac0f0c95eb5e8381ce9d2194fe38447a7d4f4a67a8ce6...      6582272   \n",
       "2     56  7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...      6291456   \n",
       "3     60  7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...      6660096   \n",
       "4     61  7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...      7077888   \n",
       "5     62  7ce8f1322c8ffe4aa635600d2e38286475832bf2b5ccc9...      6553600   \n",
       "\n",
       "  application      c_time io_zone redundancy_type op_type  \\\n",
       "0    bigtable  1705174034    WARM   ERASURE_CODED    READ   \n",
       "2    bigtable  1705046784    WARM   ERASURE_CODED    READ   \n",
       "3    bigtable  1705046784    WARM   ERASURE_CODED    READ   \n",
       "4    bigtable  1705046784    WARM   ERASURE_CODED    READ   \n",
       "5    bigtable  1705046784    WARM   ERASURE_CODED    READ   \n",
       "\n",
       "         service_class  from_flash_cache  cache_hit  request_io_size_bytes  \\\n",
       "0  THROUGHPUT_ORIENTED                 0          0                  94208   \n",
       "2  THROUGHPUT_ORIENTED                 0          0                 184320   \n",
       "3  THROUGHPUT_ORIENTED                 0          1                  77824   \n",
       "4  THROUGHPUT_ORIENTED                 1          1                 262144   \n",
       "5  THROUGHPUT_ORIENTED                 1          1                 262144   \n",
       "\n",
       "   disk_io_size_bytes  response_io_size_bytes    start_time  disk_time  \\\n",
       "0               98304                   94208  1.705651e+09   0.005128   \n",
       "2             1048576                  184320  1.705651e+09   0.004203   \n",
       "3                   0                   77824  1.705651e+09   0.000000   \n",
       "4                   0                  262144  1.705651e+09   0.000000   \n",
       "5                   0                  262144  1.705651e+09   0.000000   \n",
       "\n",
       "   simulated_disk_start_time  simulated_latency  \n",
       "0               1.705651e+09           0.005128  \n",
       "2               1.705651e+09           0.037774  \n",
       "3               0.000000e+00           0.000068  \n",
       "4               0.000000e+00           0.000077  \n",
       "5               0.000000e+00           0.000054  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find lines where request_io_size_bytes != disk_io_size_bytes\n",
    "mismatched_io_sizes = trace[trace[\"request_io_size_bytes\"] != trace[\"disk_io_size_bytes\"]]\n",
    "\n",
    "# Display the first few rows of mismatched IO sizes\n",
    "print(\"Rows where request_io_size_bytes != disk_io_size_bytes:\")\n",
    "mismatched_io_sizes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many writes don't have a corresponding read?\n",
    "\n",
    "def convert_trace_to_per_page(trace: pd.DataFrame) -> pd.DataFrame:\n",
    "    new_rows = []\n",
    "    for _, row in trace.iterrows():\n",
    "        filename = row[\"filename\"]\n",
    "        offset = row[\"file_offset\"]\n",
    "        timestamp = row[\"start_time\"]\n",
    "        io_size = row[\"request_io_size_bytes\"]\n",
    "        operation = row[\"op_type\"]\n",
    "        start_page_num = offset // PAGE_SIZE\n",
    "        end_page_num = math.ceil((offset + io_size) / PAGE_SIZE)\n",
    "        for page_num in range(start_page_num, end_page_num):\n",
    "            offset = page_num * PAGE_SIZE\n",
    "            io_size = PAGE_SIZE\n",
    "            obj_id = f\"{filename}:{offset}\"\n",
    "            new_rows.append({\n",
    "                \"timestamp\": timestamp,\n",
    "                \"obj_id\": obj_id,\n",
    "                \"obj_size\": io_size,\n",
    "                \"op_type\": operation,\n",
    "            })\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "trace_per_page_df = convert_trace_to_per_page(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of writes: 10986920\n",
      "Number of writes without corresponding reads AFTER them: 4228795 (38.5%)\n"
     ]
    }
   ],
   "source": [
    "# How many writes?\n",
    "num_writes = len(trace_per_page_df[trace_per_page_df[\"op_type\"] == \"WRITE\"])\n",
    "print(\"Number of writes: %d\" % num_writes)\n",
    "\n",
    "# How many writes don't have a corresponding read AFTER the write?\n",
    "# Get writes and reads as separate dataframes\n",
    "writes = trace_per_page_df[trace_per_page_df[\"op_type\"] == \"WRITE\"]\n",
    "reads = trace_per_page_df[trace_per_page_df[\"op_type\"] == \"READ\"]\n",
    "\n",
    "# Left join writes with reads on obj_id to find writes without matching reads\n",
    "# Only keep writes where timestamp is less than read timestamp (if read exists)\n",
    "writes_without_reads = writes.merge(\n",
    "    reads,\n",
    "    on=\"obj_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"_write\", \"_read\")\n",
    ")\n",
    "\n",
    "# Filter after merge to avoid numpy array comparison issues\n",
    "writes_without_reads = writes_without_reads[\n",
    "    writes_without_reads[\"timestamp_read\"].isna()\n",
    "]\n",
    "\n",
    "num_writes_without_reads = len(writes_without_reads)\n",
    "print(\"Number of writes without corresponding reads AFTER them: %d (%.1f%%)\" %\n",
    "      (num_writes_without_reads, 100 * num_writes_without_reads / num_writes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of writes with preceding reads: 9121500 (83.0%)\n"
     ]
    }
   ],
   "source": [
    "writes_with_preceding_reads = writes.merge(\n",
    "    reads,\n",
    "    on=\"obj_id\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_write\", \"_read\")\n",
    ")\n",
    "\n",
    "writes_with_preceding_reads = writes_with_preceding_reads[\n",
    "    (writes_with_preceding_reads[\"timestamp_write\"].isna() == False) &\n",
    "    (writes_with_preceding_reads[\"timestamp_write\"] < writes_with_preceding_reads[\"timestamp_read\"])\n",
    "]\n",
    "\n",
    "num_writes_with_preceding_reads = len(writes_with_preceding_reads)\n",
    "print(\"Number of writes with preceding reads: %d (%.1f%%)\" %\n",
    "      (num_writes_with_preceding_reads, 100 * num_writes_with_preceding_reads / num_writes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
