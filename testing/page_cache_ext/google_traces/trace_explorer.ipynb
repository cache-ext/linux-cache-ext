{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "cluster_name = \"cluster1_16TB\"\n",
    "date = \"20240115\"\n",
    "start_idx = 52\n",
    "num_traces_to_include = 6\n",
    "url_template = \"https://storage.googleapis.com/thesios-io-traces/%s/%s/%s\"\n",
    "initialize_data_dir = True\n",
    "remove_zero_size_ops = True\n",
    "data_file_from_idx = lambda idx: \"data-00%s-of-00100\" % str(idx).zfill(3)\n",
    "\n",
    "# Computed\n",
    "assert start_idx >= 0 and start_idx < 100, \"Invalid start_idx: %d\" % start_idx\n",
    "end_idx = start_idx + num_traces_to_include - 1\n",
    "assert end_idx >= 0 and end_idx < 100, \"Invalid end_idx: %d\" % end_idx\n",
    "output_trace_file = \"trace_%s_%s_from_%s_to_%s.txt\" % (cluster_name, date, start_idx, end_idx)\n",
    "data_dir = \"data_%s_%s_from_%s_to_%s_temp\" % (cluster_name, date, start_idx, end_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def download_to_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def format_bytes(bytes: int) -> str:\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if bytes < 1024:\n",
    "            return f\"{bytes:.2f} {unit}\"\n",
    "        bytes /= 1024\n",
    "    return f\"{bytes:.2f} PB\"\n",
    "\n",
    "def process_trace(trace) -> Dict[str, Dict[str, int]]:\n",
    "    # We want to process the trace, operation by operation.\n",
    "    # Let's simplify: assume everything pre-exists, and we only need to keep track of the last read/write offset for each file.\n",
    "\n",
    "    # We will keep track of the last read offset for each file.\n",
    "    # Initialize dictionaries to store file metadata\n",
    "    file_metadata = {}\n",
    "\n",
    "    for _, row in trace.iterrows():\n",
    "        filename = row[\"filename\"]\n",
    "        offset = row[\"file_offset\"]\n",
    "        io_size = row[\"request_io_size_bytes\"]\n",
    "        operation = row[\"op_type\"]\n",
    "\n",
    "        if filename not in file_metadata:\n",
    "            file_metadata[filename] = {\n",
    "                \"max_read_offset\": 0,\n",
    "                \"max_write_offset\": 0,\n",
    "                \"max_initialized_offset\": 0,\n",
    "            }\n",
    "\n",
    "        metadata = file_metadata[filename]\n",
    "\n",
    "        if operation == \"READ\":\n",
    "            metadata[\"max_initialized_offset\"] = max(metadata[\"max_initialized_offset\"], offset + io_size)\n",
    "            metadata[\"max_read_offset\"] = max(metadata[\"max_read_offset\"], offset + io_size)\n",
    "        elif operation == \"WRITE\":\n",
    "            metadata[\"max_initialized_offset\"] = max(metadata[\"max_initialized_offset\"], offset + io_size)\n",
    "            metadata[\"max_write_offset\"] = max(metadata[\"max_write_offset\"], offset + io_size)\n",
    "\n",
    "    return file_metadata\n",
    "\n",
    "def print_file_metadata_stats(file_metadata) -> Tuple[int, int, int]:\n",
    "    total_size_initialized = sum([metadata[\"max_initialized_offset\"] for metadata in file_metadata.values()])\n",
    "    total_size_read = sum([metadata[\"max_read_offset\"] for metadata in file_metadata.values()])\n",
    "    total_size_written = sum([metadata[\"max_write_offset\"] for metadata in file_metadata.values()])\n",
    "\n",
    "    print(f\"Total size initialized: {format_bytes(total_size_initialized)}\")\n",
    "    print(f\"Total size read: {format_bytes(total_size_read)}\")\n",
    "    print(f\"Total size written: {format_bytes(total_size_written)}\")\n",
    "    return total_size_initialized, total_size_read, total_size_written\n",
    "\n",
    "# Initialize a new directory and write the files needed to run the trace\n",
    "def initialize_file(file_path: str, size: int):\n",
    "    \"\"\"Initialize a file with pseudo-random data.\"\"\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        # Write data in chunks to avoid excessive memory usage\n",
    "        chunk_size = 16 * 1024 * 1024  # 16 MB chunks\n",
    "        remaining_size = size\n",
    "\n",
    "        while remaining_size > 0:\n",
    "            write_size = min(chunk_size, remaining_size)\n",
    "            data = np.random.bytes(write_size)\n",
    "            f.write(data)\n",
    "            remaining_size -= write_size\n",
    "    # print(f\"Initialized file: {file_path} with size: {format_bytes(size)}\")\n",
    "\n",
    "def initialize_file_single_arg(args):\n",
    "    return initialize_file(*args)\n",
    "\n",
    "def create_trace_data_dir(data_dir: str, file_metadata: Dict[str, Dict[str, int]]):\n",
    "    print(\"Initializing data directory: %s\" % data_dir)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    else:\n",
    "        print(\"Data directory already exists, not doing anything.\")\n",
    "        return\n",
    "\n",
    "    # Process files in parallel\n",
    "    total_files = len(file_metadata)\n",
    "    print(f\"Initializing {total_files} files...\")\n",
    "\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        tasks = []\n",
    "        for filename, metadata in file_metadata.items():\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            size = metadata[\"max_initialized_offset\"]\n",
    "            tasks.append((file_path, size))\n",
    "\n",
    "        # Use multiprocessing to parallelize file initialization\n",
    "        for _ in tqdm(pool.imap(initialize_file_single_arg, tasks), total=total_files):\n",
    "            pass\n",
    "\n",
    "    print(\"File initialization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_dfs = []\n",
    "for idx in range(start_idx, end_idx + 1):\n",
    "    data_file = data_file_from_idx(idx)\n",
    "    local_file = \"%s_%s_%s.csv\" % (cluster_name, date, data_file)\n",
    "    download_url = url_template % (cluster_name, date, data_file)\n",
    "\n",
    "    if not os.path.exists(local_file):\n",
    "        print(\"Downloading %s to %s\" % (download_url, local_file))\n",
    "        download_to_file(download_url, local_file)\n",
    "    trace_dfs.append(pd.read_csv(local_file))\n",
    "\n",
    "# Combine all the traces into a single file\n",
    "combined_trace = pd.concat(trace_dfs, ignore_index=True)\n",
    "trace = combined_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>917f44e5c65607c83f7d94334b969ce23ea742e29d66e3...</td>\n",
       "      <td>837216</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705339049</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32832</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>655858a2cfd7a28c1c9d77de3b9341ebce4331b4c07412...</td>\n",
       "      <td>4423009</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705349515</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>LATENCY_SENSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>3764</td>\n",
       "      <td>3764</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0b12b74b37fe27d1f4652b506a5f7112ac33fc93258231...</td>\n",
       "      <td>1050624</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705348783</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0b12b74b37fe27d1f4652b506a5f7112ac33fc93258231...</td>\n",
       "      <td>1050624</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705348783</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1050624</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5bf831a5fb370e7a26c40c91ea13323e13bfb08a145e0c...</td>\n",
       "      <td>3677184</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705262431</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>262656</td>\n",
       "      <td>262144</td>\n",
       "      <td>262656</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.001493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  file_offset  \\\n",
       "0  917f44e5c65607c83f7d94334b969ce23ea742e29d66e3...       837216   \n",
       "1  655858a2cfd7a28c1c9d77de3b9341ebce4331b4c07412...      4423009   \n",
       "2  0b12b74b37fe27d1f4652b506a5f7112ac33fc93258231...      1050624   \n",
       "3  0b12b74b37fe27d1f4652b506a5f7112ac33fc93258231...      1050624   \n",
       "4  5bf831a5fb370e7a26c40c91ea13323e13bfb08a145e0c...      3677184   \n",
       "\n",
       "                                         application      c_time io_zone  \\\n",
       "0  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705339049    WARM   \n",
       "1                                           bigtable  1705349515    WARM   \n",
       "2  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705348783    WARM   \n",
       "3  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705348783    WARM   \n",
       "4  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705262431    WARM   \n",
       "\n",
       "  redundancy_type op_type        service_class  from_flash_cache  cache_hit  \\\n",
       "0      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          1   \n",
       "1      REPLICATED   WRITE    LATENCY_SENSITIVE                 0         -1   \n",
       "2   ERASURE_CODED   WRITE  THROUGHPUT_ORIENTED                 0         -1   \n",
       "3   ERASURE_CODED   WRITE  THROUGHPUT_ORIENTED                 0         -1   \n",
       "4      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "\n",
       "   request_io_size_bytes  disk_io_size_bytes  response_io_size_bytes  \\\n",
       "0                  32832                   0                       0   \n",
       "1                   3764                3764                       0   \n",
       "2                      0                   0                       0   \n",
       "3                1050624             1050624                       0   \n",
       "4                 262656              262144                  262656   \n",
       "\n",
       "     start_time  disk_time  simulated_disk_start_time  simulated_latency  \n",
       "0  1.705350e+09   0.000000               0.000000e+00           0.000044  \n",
       "1  1.705350e+09   0.000000               0.000000e+00           0.000051  \n",
       "2  1.705350e+09   0.000000               0.000000e+00           0.000113  \n",
       "3  1.705350e+09   0.000000               0.000000e+00           0.000654  \n",
       "4  1.705350e+09   0.001486               1.705350e+09           0.001493  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_zero_size_ops:\n",
    "    trace = trace[trace[\"request_io_size_bytes\"] > 0].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in trace: 859080\n",
      "Read operations: 479963\n",
      "Write operations: 379117\n",
      "Read proportion: 55.87%\n",
      "Read bytes: 164.09 GB\n",
      "Write bytes: 104.85 GB\n",
      "Total bytes: 268.95 GB\n"
     ]
    }
   ],
   "source": [
    "# How many lines is the trace?\n",
    "print(\"Number of lines in trace: %d\" % len(trace))\n",
    "\n",
    "# What is the proportion of read and write operations?\n",
    "read_ops = len(trace[trace[\"op_type\"] == \"READ\"])\n",
    "write_ops = len(trace[trace[\"op_type\"] == \"WRITE\"])\n",
    "print(\"Read operations: %d\" % read_ops)\n",
    "print(\"Write operations: %d\" % write_ops)\n",
    "print(\"Read proportion: %.2f%%\" % (100 * read_ops / (read_ops + write_ops)))\n",
    "\n",
    "read_bytes = trace[trace[\"op_type\"] == \"READ\"][\"request_io_size_bytes\"].sum()\n",
    "write_bytes = trace[trace[\"op_type\"] == \"WRITE\"][\"request_io_size_bytes\"].sum()\n",
    "total_bytes = read_bytes + write_bytes\n",
    "print(\"Read bytes: %s\" % format_bytes(read_bytes))\n",
    "print(\"Write bytes: %s\" % format_bytes(write_bytes))\n",
    "print(\"Total bytes: %s\" % format_bytes(total_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique filenames: 66451\n",
      "\n",
      "Unique filenames:\n",
      "['917f44e5c65607c83f7d94334b969ce23ea742e29d66e307b838583614cc5356'\n",
      " '655858a2cfd7a28c1c9d77de3b9341ebce4331b4c0741262c1b68911d49901cc'\n",
      " '0b12b74b37fe27d1f4652b506a5f7112ac33fc932582315421ccfa42b3a61e7b' ...\n",
      " 'b08af0d890cd66732f0e79a306b0471df8dacf4e135ce52900ff36e6f25b5c9b'\n",
      " 'dbb6007db05c3fe96df6368e2805a47957b4214f076234a439e59fb795741709'\n",
      " '1674b3a012914f8520433d1f580c065e39fdf3541ae22954c326ea597393d213']\n"
     ]
    }
   ],
   "source": [
    "num_unique_filenames = trace[\"filename\"].nunique()\n",
    "print(f\"Number of unique filenames: {num_unique_filenames}\")\n",
    "print(\"\\nUnique filenames:\")\n",
    "print(trace[\"filename\"].unique())\n",
    "if num_unique_filenames > 80000:\n",
    "    raise Exception(\"Too many unique filenames: %d\" % num_unique_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size initialized: 318.89 GB\n",
      "Total size read: 268.82 GB\n",
      "Total size written: 104.98 GB\n"
     ]
    }
   ],
   "source": [
    "file_metadata = process_trace(trace)\n",
    "total_size_initialized, total_size_read, total_size_written = print_file_metadata_stats(file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "GiB = 2 ** 30\n",
    "if total_size_initialized > 400 * GiB:\n",
    "    raise Exception(\"Too much data initialized: %s\" % format_bytes(total_size_initialized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data directory: data_cluster1_16TB_20240115_from_52_to_57_temp\n",
      "Initializing 66451 files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3040/2534172458.py:94: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for _ in tqdm(pool.imap(initialize_file_single_arg, tasks), total=total_files):\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.031078338623046875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 66451,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f43b57c3d734c01bc8f7ad9175a4b28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/66451 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File initialization complete.\n"
     ]
    }
   ],
   "source": [
    "if initialize_data_dir:\n",
    "    create_trace_data_dir(data_dir, file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace file already exists: trace_cluster1_16TB_20240115_from_52_to_57.txt\n",
      "Not overwriting.\n"
     ]
    }
   ],
   "source": [
    "# Format the trace file to be used in the benchmark client\n",
    "# The expected format is:\n",
    "# <op_type> <filename>:<offset>:<size>\n",
    "# Example:\n",
    "# READ file1:0:4096\n",
    "# WRITE file2:0:4096\n",
    "\n",
    "def convert_trace_to_bench_client_format(trace: pd.DataFrame, output_file: str):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _, row in trace.iterrows():\n",
    "            filename = row[\"filename\"]\n",
    "            offset = row[\"file_offset\"]\n",
    "            io_size = row[\"request_io_size_bytes\"]\n",
    "            operation = row[\"op_type\"]\n",
    "\n",
    "            f.write(f\"{operation} {filename}:{offset}:{io_size}\\n\")\n",
    "\n",
    "if not os.path.exists(output_trace_file):\n",
    "    print(\"Converting trace to benchmark client format...\")\n",
    "    convert_trace_to_bench_client_format(trace, output_trace_file)\n",
    "    print(\"Trace file converted to benchmark client format: %s\" % output_trace_file)\n",
    "else:\n",
    "    print(\"Trace file already exists: %s\" % output_trace_file)\n",
    "    print(\"Not overwriting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where request_io_size_bytes != disk_io_size_bytes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>917f44e5c65607c83f7d94334b969ce23ea742e29d66e3...</td>\n",
       "      <td>837216</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705339049</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32832</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5bf831a5fb370e7a26c40c91ea13323e13bfb08a145e0c...</td>\n",
       "      <td>3677184</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705262431</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>262656</td>\n",
       "      <td>262144</td>\n",
       "      <td>262656</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.001493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>8faac84afe631d341f8a8fdd41c546bd71abb424fdccf3...</td>\n",
       "      <td>8339328</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705327243</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>16416</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>8faac84afe631d341f8a8fdd41c546bd71abb424fdccf3...</td>\n",
       "      <td>8339328</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705327243</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>65664</td>\n",
       "      <td>73728</td>\n",
       "      <td>65664</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.005201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1c6d2ba3324fd75086f09f92b91e00fef488ebe1853dbb...</td>\n",
       "      <td>3145728</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705237221</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>270336</td>\n",
       "      <td>0</td>\n",
       "      <td>270336</td>\n",
       "      <td>1.705350e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           filename  file_offset  \\\n",
       "0      0  917f44e5c65607c83f7d94334b969ce23ea742e29d66e3...       837216   \n",
       "3      4  5bf831a5fb370e7a26c40c91ea13323e13bfb08a145e0c...      3677184   \n",
       "4      5  8faac84afe631d341f8a8fdd41c546bd71abb424fdccf3...      8339328   \n",
       "5      6  8faac84afe631d341f8a8fdd41c546bd71abb424fdccf3...      8339328   \n",
       "8      9  1c6d2ba3324fd75086f09f92b91e00fef488ebe1853dbb...      3145728   \n",
       "\n",
       "                                         application      c_time io_zone  \\\n",
       "0  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705339049    WARM   \n",
       "3  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705262431    WARM   \n",
       "4  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705327243    WARM   \n",
       "5  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705327243    WARM   \n",
       "8                                           bigtable  1705237221    WARM   \n",
       "\n",
       "  redundancy_type op_type        service_class  from_flash_cache  cache_hit  \\\n",
       "0      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          1   \n",
       "3      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "4      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          1   \n",
       "5      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "8   ERASURE_CODED    READ  THROUGHPUT_ORIENTED                 0          1   \n",
       "\n",
       "   request_io_size_bytes  disk_io_size_bytes  response_io_size_bytes  \\\n",
       "0                  32832                   0                       0   \n",
       "3                 262656              262144                  262656   \n",
       "4                  16416                   0                       0   \n",
       "5                  65664               73728                   65664   \n",
       "8                 270336                   0                  270336   \n",
       "\n",
       "     start_time  disk_time  simulated_disk_start_time  simulated_latency  \n",
       "0  1.705350e+09   0.000000               0.000000e+00           0.000044  \n",
       "3  1.705350e+09   0.001486               1.705350e+09           0.001493  \n",
       "4  1.705350e+09   0.000000               0.000000e+00           0.000165  \n",
       "5  1.705350e+09   0.005166               1.705350e+09           0.005201  \n",
       "8  1.705350e+09   0.000000               0.000000e+00           0.000068  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find lines where request_io_size_bytes != disk_io_size_bytes\n",
    "mismatched_io_sizes = trace[trace[\"request_io_size_bytes\"] != trace[\"disk_io_size_bytes\"]]\n",
    "\n",
    "# Display the first few rows of mismatched IO sizes\n",
    "print(\"Rows where request_io_size_bytes != disk_io_size_bytes:\")\n",
    "mismatched_io_sizes.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
