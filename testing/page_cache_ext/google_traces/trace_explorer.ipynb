{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import math\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from subprocess import run\n",
    "import multiprocessing as mp\n",
    "\n",
    "from typing import List, Dict, Any, Tuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "cluster_name = \"cluster2_16TB\"\n",
    "date = \"20240119\"\n",
    "start_idx = 0\n",
    "num_traces_to_include = 100\n",
    "url_template = \"https://storage.googleapis.com/thesios-io-traces/%s/%s/%s\"\n",
    "initialize_data_dir = False\n",
    "remove_zero_size_ops = True\n",
    "data_file_from_idx = lambda idx: \"data-00%s-of-00100\" % str(idx).zfill(3)\n",
    "app_to_keep = \"3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59b581e022d547290d69\"\n",
    "num_ops_to_keep = 700000\n",
    "trace_folder = \"traces\"\n",
    "\n",
    "# Computed\n",
    "def format_app_to_keep(s: str) -> str:\n",
    "    if len(s) < 12:\n",
    "        return s\n",
    "    return s[:8]\n",
    "\n",
    "assert start_idx >= 0 and start_idx <= 100, \"Invalid start_idx: %d\" % start_idx\n",
    "end_idx = start_idx + num_traces_to_include - 1\n",
    "assert end_idx >= 0 and end_idx <= 100, \"Invalid end_idx: %d\" % end_idx\n",
    "output_trace_file = \"trace_%s_%s\" % (cluster_name, date)\n",
    "if app_to_keep:\n",
    "    output_trace_file += \"_app_%s\" % format_app_to_keep(app_to_keep)\n",
    "output_trace_file += \".txt\"\n",
    "data_dir = \"data_%s_%s\" % (cluster_name, date)\n",
    "if app_to_keep:\n",
    "    data_dir += \"_app_%s\" % format_app_to_keep(app_to_keep)\n",
    "data_dir += \"_temp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(trace_folder):\n",
    "    os.makedirs(trace_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def download_to_file_single_arg(args):\n",
    "    return download_to_file(*args)\n",
    "\n",
    "def download_to_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def format_bytes(bytes: int) -> str:\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if bytes < 1024:\n",
    "            return f\"{bytes:.2f} {unit}\"\n",
    "        bytes /= 1024\n",
    "    return f\"{bytes:.2f} PB\"\n",
    "\n",
    "def process_trace(trace) -> Dict[str, Dict[str, int]]:\n",
    "    # We want to process the trace, operation by operation.\n",
    "    # Let's simplify: assume everything pre-exists, and we only need to keep track of the last read/write offset for each file.\n",
    "\n",
    "    # We will keep track of the last read offset for each file.\n",
    "    # Initialize dictionaries to store file metadata\n",
    "    file_metadata = {}\n",
    "\n",
    "    for _, row in trace.iterrows():\n",
    "        filename = row[\"filename\"]\n",
    "        offset = row[\"file_offset\"]\n",
    "        io_size = row[\"request_io_size_bytes\"]\n",
    "        operation = row[\"op_type\"]\n",
    "\n",
    "        if filename not in file_metadata:\n",
    "            file_metadata[filename] = {\n",
    "                \"max_read_offset\": 0,\n",
    "                \"max_write_offset\": 0,\n",
    "                \"max_initialized_offset\": 0,\n",
    "            }\n",
    "\n",
    "        metadata = file_metadata[filename]\n",
    "\n",
    "        if operation == \"READ\":\n",
    "            metadata[\"max_initialized_offset\"] = max(metadata[\"max_initialized_offset\"], offset + io_size)\n",
    "            metadata[\"max_read_offset\"] = max(metadata[\"max_read_offset\"], offset + io_size)\n",
    "        elif operation == \"WRITE\":\n",
    "            metadata[\"max_initialized_offset\"] = max(metadata[\"max_initialized_offset\"], offset + io_size)\n",
    "            metadata[\"max_write_offset\"] = max(metadata[\"max_write_offset\"], offset + io_size)\n",
    "\n",
    "    return file_metadata\n",
    "\n",
    "def print_file_metadata_stats(file_metadata) -> Tuple[int, int, int]:\n",
    "    total_size_initialized = sum([metadata[\"max_initialized_offset\"] for metadata in file_metadata.values()])\n",
    "    total_size_read = sum([metadata[\"max_read_offset\"] for metadata in file_metadata.values()])\n",
    "    total_size_written = sum([metadata[\"max_write_offset\"] for metadata in file_metadata.values()])\n",
    "\n",
    "    print(f\"Total size initialized: {format_bytes(total_size_initialized)}\")\n",
    "    print(f\"Total size read: {format_bytes(total_size_read)}\")\n",
    "    print(f\"Total size written: {format_bytes(total_size_written)}\")\n",
    "    return total_size_initialized, total_size_read, total_size_written\n",
    "\n",
    "# Initialize a new directory and write the files needed to run the trace\n",
    "def initialize_file(file_path: str, size: int):\n",
    "    \"\"\"Initialize a file with pseudo-random data.\"\"\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        # Write data in chunks to avoid excessive memory usage\n",
    "        chunk_size = 16 * 1024 * 1024  # 16 MB chunks\n",
    "        remaining_size = size\n",
    "\n",
    "        while remaining_size > 0:\n",
    "            write_size = min(chunk_size, remaining_size)\n",
    "            data = np.random.bytes(write_size)\n",
    "            f.write(data)\n",
    "            remaining_size -= write_size\n",
    "    # print(f\"Initialized file: {file_path} with size: {format_bytes(size)}\")\n",
    "\n",
    "def initialize_file_single_arg(args):\n",
    "    return initialize_file(*args)\n",
    "\n",
    "def create_trace_data_dir(data_dir: str, file_metadata: Dict[str, Dict[str, int]]):\n",
    "    print(\"Initializing data directory: %s\" % data_dir)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    else:\n",
    "        print(\"Data directory already exists, not doing anything.\")\n",
    "        return\n",
    "\n",
    "    # Process files in parallel\n",
    "    total_files = len(file_metadata)\n",
    "    print(f\"Initializing {total_files} files...\")\n",
    "\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        tasks = []\n",
    "        for filename, metadata in file_metadata.items():\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            size = metadata[\"max_initialized_offset\"]\n",
    "            tasks.append((file_path, size))\n",
    "\n",
    "        # Use multiprocessing to parallelize file initialization\n",
    "        for _ in tqdm(pool.imap(initialize_file_single_arg, tasks), total=total_files):\n",
    "            pass\n",
    "\n",
    "    print(\"File initialization complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_files = []\n",
    "download_tasks = []\n",
    "for idx in range(start_idx, end_idx + 1):\n",
    "    data_file = data_file_from_idx(idx)\n",
    "    local_file = \"%s_%s_%s.csv\" % (cluster_name, date, data_file)\n",
    "    local_file = os.path.join(trace_folder, local_file)\n",
    "    download_url = url_template % (cluster_name, date, data_file)\n",
    "    local_files.append(local_file)\n",
    "\n",
    "    if not os.path.exists(local_file):\n",
    "        print(\"Downloading %s to %s\" % (download_url, local_file))\n",
    "        download_tasks.append((download_url, local_file))\n",
    "\n",
    "if len(download_tasks) > 0:\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        for _ in tqdm(pool.imap(download_to_file_single_arg, download_tasks), total=len(download_tasks)):\n",
    "            pass\n",
    "\n",
    "# Combine all the traces into a single file\n",
    "trace_dfs = [ pd.read_csv(local_file) for local_file in local_files ]\n",
    "combined_trace = pd.concat(trace_dfs, ignore_index=True)\n",
    "trace = combined_trace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5def9de616cdd1939104e38cc9feb4f049ec7fad38ab6c...</td>\n",
       "      <td>4202496</td>\n",
       "      <td>08d81b9cb3e4dcc7e55af443437cf6bf7be30b970a31bf...</td>\n",
       "      <td>1705651199</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5def9de616cdd1939104e38cc9feb4f049ec7fad38ab6c...</td>\n",
       "      <td>4202496</td>\n",
       "      <td>08d81b9cb3e4dcc7e55af443437cf6bf7be30b970a31bf...</td>\n",
       "      <td>1705651199</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1050624</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>e9eb9de6ce9e393bf9e7a69cdfb8fd2bb1b7891626b142...</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1c0fab5e9c61ea6f0fd93b1ef13442b30914bd61b7c246...</td>\n",
       "      <td>1705649755</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>430080</td>\n",
       "      <td>1056768</td>\n",
       "      <td>430080</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.01718</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.017180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0ea93d8ddb4672ccf2a422ec3297b85f66b534fef60101...</td>\n",
       "      <td>6093361</td>\n",
       "      <td>c994007e4688d5bb2a549cdcf077c20baa9da6d949e893...</td>\n",
       "      <td>1705629496</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>LATENCY_SENSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0ea93d8ddb4672ccf2a422ec3297b85f66b534fef60101...</td>\n",
       "      <td>6093361</td>\n",
       "      <td>c994007e4688d5bb2a549cdcf077c20baa9da6d949e893...</td>\n",
       "      <td>1705629496</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>LATENCY_SENSITIVE</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>363018</td>\n",
       "      <td>363018</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.010792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  file_offset  \\\n",
       "0  5def9de616cdd1939104e38cc9feb4f049ec7fad38ab6c...      4202496   \n",
       "1  5def9de616cdd1939104e38cc9feb4f049ec7fad38ab6c...      4202496   \n",
       "2  e9eb9de6ce9e393bf9e7a69cdfb8fd2bb1b7891626b142...      1050624   \n",
       "3  0ea93d8ddb4672ccf2a422ec3297b85f66b534fef60101...      6093361   \n",
       "4  0ea93d8ddb4672ccf2a422ec3297b85f66b534fef60101...      6093361   \n",
       "\n",
       "                                         application      c_time io_zone  \\\n",
       "0  08d81b9cb3e4dcc7e55af443437cf6bf7be30b970a31bf...  1705651199    WARM   \n",
       "1  08d81b9cb3e4dcc7e55af443437cf6bf7be30b970a31bf...  1705651199    WARM   \n",
       "2  1c0fab5e9c61ea6f0fd93b1ef13442b30914bd61b7c246...  1705649755    WARM   \n",
       "3  c994007e4688d5bb2a549cdcf077c20baa9da6d949e893...  1705629496    WARM   \n",
       "4  c994007e4688d5bb2a549cdcf077c20baa9da6d949e893...  1705629496    WARM   \n",
       "\n",
       "  redundancy_type op_type      service_class  from_flash_cache  cache_hit  \\\n",
       "0   ERASURE_CODED   WRITE              OTHER                 0         -1   \n",
       "1   ERASURE_CODED   WRITE              OTHER                 0         -1   \n",
       "2   ERASURE_CODED    READ              OTHER                 0          0   \n",
       "3      REPLICATED   WRITE  LATENCY_SENSITIVE                 0         -1   \n",
       "4      REPLICATED   WRITE  LATENCY_SENSITIVE                 0         -1   \n",
       "\n",
       "   request_io_size_bytes  disk_io_size_bytes  response_io_size_bytes  \\\n",
       "0                      0                   0                       0   \n",
       "1                1050624             1050624                       0   \n",
       "2                 430080             1056768                  430080   \n",
       "3                      0                   0                       0   \n",
       "4                 363018              363018                       0   \n",
       "\n",
       "     start_time  disk_time  simulated_disk_start_time  simulated_latency  \n",
       "0  1.705651e+09    0.00000               0.000000e+00           0.000043  \n",
       "1  1.705651e+09    0.00000               0.000000e+00           0.000433  \n",
       "2  1.705651e+09    0.01718               1.705651e+09           0.017180  \n",
       "3  1.705651e+09    0.00000               0.000000e+00           0.000086  \n",
       "4  1.705651e+09    0.00000               0.000000e+00           0.010792  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique applications: 849\n",
      "Number of operations per application:\n",
      "application\n",
      "c994007e4688d5bb2a549cdcf077c20baa9da6d949e893173f642c66047e3c3c    3451429\n",
      "3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59b581e022d547290d69     867452\n",
      "7e9de8e118680d9d454a63c405a6ba56475db9f57e27f3fcef6f722cc8b6d406     601510\n",
      "93ad3bf6273c13fcaa8f8b1793a666f22aa75d976b099cdfdacacc0ccb6c64ff     470144\n",
      "89bbe87e189233e198c0913677ecb8eb6bd0e6c651e3ba236e15863428894553     305636\n",
      "                                                                     ...   \n",
      "3262ffeffa1f934ded2b1c7455c1946e2bdf9c4fcb1daa1e9e2995f6f77e12c2          1\n",
      "f7e2f5a011c70d41ce6ee8654a65f0145b51e0f28d9b7f02b490d064f7263b55          1\n",
      "ee0e30dce552021557ad0456c79474ead4c35f1f2a959d6cb0a617b8e5c4aac6          1\n",
      "f6aadc7f500075f82b060507e4bef6f01b5b073b6e39dbcfaa340a82912edc6e          1\n",
      "88981a098017224b43c68cbee1067e95f52ce2d779de254f92687006fcb64066          1\n",
      "Length: 849, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print unique applications\n",
    "print(\"Number of unique applications: %d\" % len(trace[\"application\"].unique()))\n",
    "\n",
    "# Print number of operations per application\n",
    "operations_per_application = trace.groupby(\"application\").size().sort_values(ascending=False)\n",
    "print(\"Number of operations per application:\")\n",
    "print(operations_per_application)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if remove_zero_size_ops:\n",
    "    trace = trace[trace[\"request_io_size_bytes\"] > 0].reset_index()\n",
    "if app_to_keep and len(app_to_keep) > 0:\n",
    "    trace = trace[trace[\"application\"] == app_to_keep].head(num_ops_to_keep).reset_index(drop=True).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in trace: 583961\n",
      "Read operations: 258466\n",
      "Write operations: 325495\n",
      "Read proportion: 44.26%\n",
      "Read bytes: 249.38 GB\n",
      "Write bytes: 228.60 GB\n",
      "Total bytes: 477.98 GB\n"
     ]
    }
   ],
   "source": [
    "# How many lines is the trace?\n",
    "print(\"Number of lines in trace: %d\" % len(trace))\n",
    "\n",
    "# What is the proportion of read and write operations?\n",
    "read_ops = len(trace[trace[\"op_type\"] == \"READ\"])\n",
    "write_ops = len(trace[trace[\"op_type\"] == \"WRITE\"])\n",
    "print(\"Read operations: %d\" % read_ops)\n",
    "print(\"Write operations: %d\" % write_ops)\n",
    "print(\"Read proportion: %.2f%%\" % (100 * read_ops / (read_ops + write_ops)))\n",
    "\n",
    "read_bytes = trace[trace[\"op_type\"] == \"READ\"][\"request_io_size_bytes\"].sum()\n",
    "write_bytes = trace[trace[\"op_type\"] == \"WRITE\"][\"request_io_size_bytes\"].sum()\n",
    "total_bytes = read_bytes + write_bytes\n",
    "print(\"Read bytes: %s\" % format_bytes(read_bytes))\n",
    "print(\"Write bytes: %s\" % format_bytes(write_bytes))\n",
    "print(\"Total bytes: %s\" % format_bytes(total_bytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique filenames: 30027\n",
      "\n",
      "Unique filenames:\n",
      "['bc51c4adbb722097b5d5d6740e50a55a0302b453623dd364cb0ca35aa410c7a2'\n",
      " '334d52f28ddcc13458079787436d6586543a6a45a4ba242a0a73cf46ab776d2f'\n",
      " 'a143529558261b9f95d760d62bc753ef85954cb52a2b9a6597d28ee7ab179c31' ...\n",
      " '5e76cd9798302b3fc243a5ba1c932c244f4272cccd3324eb99894ab4cc2f4566'\n",
      " 'de0d256c816ed204956c4eea58e0284fa9cfc49cb930ddd26c4100d7b344d418'\n",
      " '327cabe75cb37bcb166fe41568ba4d028e88aae6def7b202420c6389ce6a4599']\n"
     ]
    }
   ],
   "source": [
    "num_unique_filenames = trace[\"filename\"].nunique()\n",
    "print(f\"Number of unique filenames: {num_unique_filenames}\")\n",
    "print(\"\\nUnique filenames:\")\n",
    "print(trace[\"filename\"].unique())\n",
    "if num_unique_filenames > 100000:\n",
    "    raise Exception(\"Too many unique filenames: %d\" % num_unique_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size initialized: 230.22 GB\n",
      "Total size read: 195.79 GB\n",
      "Total size written: 228.61 GB\n"
     ]
    }
   ],
   "source": [
    "file_metadata = process_trace(trace)\n",
    "total_size_initialized, total_size_read, total_size_written = print_file_metadata_stats(file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "GiB = 2 ** 30\n",
    "if total_size_initialized > 350 * GiB:\n",
    "    raise Exception(\"Too much data initialized: %s\" % format_bytes(total_size_initialized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if initialize_data_dir:\n",
    "    create_trace_data_dir(data_dir, file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>bc51c4adbb722097b5d5d6740e50a55a0302b453623dd3...</td>\n",
       "      <td>5790005</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705629339</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>16015</td>\n",
       "      <td>16015</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127</td>\n",
       "      <td>334d52f28ddcc13458079787436d6586543a6a45a4ba24...</td>\n",
       "      <td>29417472</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705648549</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2101248</td>\n",
       "      <td>2101248</td>\n",
       "      <td>2101248</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.009916</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.089438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>152</td>\n",
       "      <td>bc51c4adbb722097b5d5d6740e50a55a0302b453623dd3...</td>\n",
       "      <td>5806020</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705629339</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>13338</td>\n",
       "      <td>13338</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>225</td>\n",
       "      <td>bc51c4adbb722097b5d5d6740e50a55a0302b453623dd3...</td>\n",
       "      <td>5819358</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705629339</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>9720</td>\n",
       "      <td>9720</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231</td>\n",
       "      <td>334d52f28ddcc13458079787436d6586543a6a45a4ba24...</td>\n",
       "      <td>31518720</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705648549</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2101248</td>\n",
       "      <td>2105344</td>\n",
       "      <td>2101248</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.024331</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.024331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           filename  file_offset  \\\n",
       "0     49  bc51c4adbb722097b5d5d6740e50a55a0302b453623dd3...      5790005   \n",
       "1    127  334d52f28ddcc13458079787436d6586543a6a45a4ba24...     29417472   \n",
       "2    152  bc51c4adbb722097b5d5d6740e50a55a0302b453623dd3...      5806020   \n",
       "3    225  bc51c4adbb722097b5d5d6740e50a55a0302b453623dd3...      5819358   \n",
       "4    231  334d52f28ddcc13458079787436d6586543a6a45a4ba24...     31518720   \n",
       "\n",
       "                                         application      c_time io_zone  \\\n",
       "0  3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705629339    WARM   \n",
       "1  3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705648549    WARM   \n",
       "2  3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705629339    WARM   \n",
       "3  3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705629339    WARM   \n",
       "4  3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705648549    WARM   \n",
       "\n",
       "  redundancy_type op_type        service_class  from_flash_cache  cache_hit  \\\n",
       "0      REPLICATED   WRITE                OTHER                 0         -1   \n",
       "1      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "2      REPLICATED   WRITE                OTHER                 0         -1   \n",
       "3      REPLICATED   WRITE                OTHER                 0         -1   \n",
       "4      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "\n",
       "   request_io_size_bytes  disk_io_size_bytes  response_io_size_bytes  \\\n",
       "0                  16015               16015                       0   \n",
       "1                2101248             2101248                 2101248   \n",
       "2                  13338               13338                       0   \n",
       "3                   9720                9720                       0   \n",
       "4                2101248             2105344                 2101248   \n",
       "\n",
       "     start_time  disk_time  simulated_disk_start_time  simulated_latency  \n",
       "0  1.705651e+09   0.000000               0.000000e+00           0.000283  \n",
       "1  1.705651e+09   0.009916               1.705651e+09           0.089438  \n",
       "2  1.705651e+09   0.000000               0.000000e+00           0.000071  \n",
       "3  1.705651e+09   0.000000               0.000000e+00           0.000073  \n",
       "4  1.705651e+09   0.024331               1.705651e+09           0.024331  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting trace to benchmark client format...\n",
      "Trace file converted to benchmark client format: trace_cluster2_16TB_20240119_app_3f524cdc.txt\n"
     ]
    }
   ],
   "source": [
    "# Format the trace file to be used in the benchmark client\n",
    "# The expected format is:\n",
    "# <op_type> <filename>:<offset>:<size>\n",
    "# Example:\n",
    "# READ file1:0:4096\n",
    "# WRITE file2:0:4096\n",
    "\n",
    "def convert_trace_to_bench_client_format(trace: pd.DataFrame, output_file: str):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _, row in trace.iterrows():\n",
    "            filename = row[\"filename\"]\n",
    "            offset = row[\"file_offset\"]\n",
    "            io_size = row[\"request_io_size_bytes\"]\n",
    "            operation = row[\"op_type\"]\n",
    "\n",
    "            f.write(f\"{operation} {filename}:{offset}:{io_size}\\n\")\n",
    "\n",
    "if not os.path.exists(output_trace_file):\n",
    "    print(\"Converting trace to benchmark client format...\")\n",
    "    convert_trace_to_bench_client_format(trace, output_trace_file)\n",
    "    print(\"Trace file converted to benchmark client format: %s\" % output_trace_file)\n",
    "else:\n",
    "    print(\"Trace file already exists: %s\" % output_trace_file)\n",
    "    print(\"Not overwriting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting trace to libcachesim format...\n",
      "trace file converted to libcachesim format: trace_cluster2_16TB_20240119_app_3f524cdc_libcachesim.csv\n",
      "Converting trace to libcachesim binary format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[INFO]  11-08-2024 14:39:55 cli_parser.cpp:231  (tid=139864297411904): trace path: trace_cluster2_16TB_20240119_app_3f524cdc_libcachesim.csv, trace_type CSV_TRACE, trace type params: time-col=1, obj-id-col=2, obj-size-col=3, delimiter=,, has-header=true\n",
      "\u001b[0m\u001b[32m[INFO]  11-08-2024 14:45:53 traceConv.cpp:63   (tid=139864297411904): trace_cluster2_16TB_20240119_app_3f524cdc_libcachesim.csv: 125.64 M requests in total\n",
      "\u001b[0m\u001b[32m[INFO]  11-08-2024 15:02:45 traceConv.cpp:95   (tid=139864297411904): trace_cluster2_16TB_20240119_app_3f524cdc_libcachesim.csv: 100 M requests (381.47 GB), trace time 67637, working set 47656072 object, 195199270912 B (181.79 GB)\n",
      "\u001b[0m\u001b[32m[INFO]  11-08-2024 15:07:03 traceConv.cpp:122  (tid=139864297411904): trace_cluster2_16TB_20240119_app_3f524cdc_libcachesim.csv: 125 M requests (479.29 GB), trace time -94148964451233, working set 60320626 object, 247073284096 B (230.10 GB), reversing output...\n",
      "\u001b[0m\u001b[32m[INFO]  11-08-2024 15:07:09 traceConv.cpp:246  (tid=139864297411904): trace_cluster2_16TB_20240119_app_3f524cdc_libcachesim.csv.oracleGeneral: 100 M requests\n",
      "\u001b[0m\u001b[32m[INFO]  11-08-2024 15:07:10 traceConv.cpp:258  (tid=139864297411904): trace conversion finished, 125642002 requests 60320626 objects, output trace_cluster2_16TB_20240119_app_3f524cdc_libcachesim.csv.oracleGeneral\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trace file converted to libcachesim binary format: trace_cluster2_16TB_20240119_app_3f524cdc_libcachesim.csv.oracleGeneral\n"
     ]
    }
   ],
   "source": [
    "PAGE_SIZE = 4096\n",
    "\n",
    "def convert_trace_to_libcachesim_format(trace: pd.DataFrame, output_file: str):\n",
    "\n",
    "    with open(output_file, \"w\") as f:\n",
    "        f.write(\"time,obj_id,obj_size\\n\")\n",
    "        for _, row in trace.iterrows():\n",
    "            filename = row[\"filename\"]\n",
    "            offset = row[\"file_offset\"]\n",
    "            timestamp = row[\"start_time\"]\n",
    "            io_size = row[\"request_io_size_bytes\"]\n",
    "            start_page_num = offset // PAGE_SIZE\n",
    "            end_page_num = math.ceil((offset + io_size) / PAGE_SIZE)\n",
    "            for page_num in range(start_page_num, end_page_num):\n",
    "                offset = page_num * PAGE_SIZE\n",
    "                io_size = PAGE_SIZE\n",
    "                obj_id = f\"{filename}:{offset}\"\n",
    "                f.write(f\"{timestamp},{obj_id},{io_size}\\n\")\n",
    "\n",
    "prefix = output_trace_file\n",
    "if prefix.endswith(\".txt\"):\n",
    "    prefix = prefix[:-4]\n",
    "output_trace_file_libcachesim = prefix + \"_libcachesim.csv\"\n",
    "if not os.path.exists(output_trace_file_libcachesim):\n",
    "    print(\"Converting trace to libcachesim format...\")\n",
    "    convert_trace_to_libcachesim_format(trace, output_trace_file_libcachesim)\n",
    "    print(\"trace file converted to libcachesim format: %s\" % output_trace_file_libcachesim)\n",
    "else:\n",
    "    print(\"libcachesim trace file already exists: %s\" % output_trace_file_libcachesim)\n",
    "    print(\"Not overwriting.\")\n",
    "\n",
    "\n",
    "output_trace_file_libcachesim_binary = output_trace_file_libcachesim + \".oracleGeneral\"\n",
    "if not os.path.exists(output_trace_file_libcachesim_binary):\n",
    "    print(\"Converting trace to libcachesim binary format...\")\n",
    "    trace_conv_bin = \"/mydata/libCacheSim/_build/bin/traceConv\"\n",
    "    if not os.path.exists(trace_conv_bin):\n",
    "        raise Exception(\"traceConv binary does not exist: %s\" % trace_conv_bin)\n",
    "    csv_opts = \"time-col=1, obj-id-col=2, obj-size-col=3, delimiter=,, has-header=true\"\n",
    "    cmd = [trace_conv_bin, output_trace_file_libcachesim, \"csv\", \"-t\", csv_opts]\n",
    "    run(cmd)\n",
    "    print(\"trace file converted to libcachesim binary format: %s\" % output_trace_file_libcachesim_binary)\n",
    "else:\n",
    "    print(\"libcachesim bninary trace file already exists: %s\" % output_trace_file_libcachesim_binary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where request_io_size_bytes != disk_io_size_bytes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>231</td>\n",
       "      <td>334d52f28ddcc13458079787436d6586543a6a45a4ba24...</td>\n",
       "      <td>31518720</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705648549</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2101248</td>\n",
       "      <td>2105344</td>\n",
       "      <td>2101248</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.024331</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.024331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>386</td>\n",
       "      <td>334d52f28ddcc13458079787436d6586543a6a45a4ba24...</td>\n",
       "      <td>35721216</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705648549</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2101248</td>\n",
       "      <td>2105344</td>\n",
       "      <td>2101248</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.022177</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.022219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>405</td>\n",
       "      <td>a143529558261b9f95d760d62bc753ef85954cb52a2b9a...</td>\n",
       "      <td>1050624</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705650838</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>595968</td>\n",
       "      <td>602112</td>\n",
       "      <td>595968</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.014296</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.022258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>423</td>\n",
       "      <td>a143529558261b9f95d760d62bc753ef85954cb52a2b9a...</td>\n",
       "      <td>1622016</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705650838</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>479232</td>\n",
       "      <td>454656</td>\n",
       "      <td>479232</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.003175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>631</td>\n",
       "      <td>334d52f28ddcc13458079787436d6586543a6a45a4ba24...</td>\n",
       "      <td>39923712</td>\n",
       "      <td>3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...</td>\n",
       "      <td>1705648549</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2101248</td>\n",
       "      <td>2105344</td>\n",
       "      <td>2101248</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.007642</td>\n",
       "      <td>1.705651e+09</td>\n",
       "      <td>0.007841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                           filename  file_offset  \\\n",
       "4     231  334d52f28ddcc13458079787436d6586543a6a45a4ba24...     31518720   \n",
       "8     386  334d52f28ddcc13458079787436d6586543a6a45a4ba24...     35721216   \n",
       "9     405  a143529558261b9f95d760d62bc753ef85954cb52a2b9a...      1050624   \n",
       "11    423  a143529558261b9f95d760d62bc753ef85954cb52a2b9a...      1622016   \n",
       "15    631  334d52f28ddcc13458079787436d6586543a6a45a4ba24...     39923712   \n",
       "\n",
       "                                          application      c_time io_zone  \\\n",
       "4   3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705648549    WARM   \n",
       "8   3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705648549    WARM   \n",
       "9   3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705650838    WARM   \n",
       "11  3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705650838    WARM   \n",
       "15  3f524cdc07a11d7c6220bdb049fe8dd41b27483c96cc59...  1705648549    WARM   \n",
       "\n",
       "   redundancy_type op_type        service_class  from_flash_cache  cache_hit  \\\n",
       "4       REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "8       REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "9    ERASURE_CODED    READ                OTHER                 0          0   \n",
       "11   ERASURE_CODED    READ                OTHER                 0          0   \n",
       "15      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "\n",
       "    request_io_size_bytes  disk_io_size_bytes  response_io_size_bytes  \\\n",
       "4                 2101248             2105344                 2101248   \n",
       "8                 2101248             2105344                 2101248   \n",
       "9                  595968              602112                  595968   \n",
       "11                 479232              454656                  479232   \n",
       "15                2101248             2105344                 2101248   \n",
       "\n",
       "      start_time  disk_time  simulated_disk_start_time  simulated_latency  \n",
       "4   1.705651e+09   0.024331               1.705651e+09           0.024331  \n",
       "8   1.705651e+09   0.022177               1.705651e+09           0.022219  \n",
       "9   1.705651e+09   0.014296               1.705651e+09           0.022258  \n",
       "11  1.705651e+09   0.001657               1.705651e+09           0.003175  \n",
       "15  1.705651e+09   0.007642               1.705651e+09           0.007841  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find lines where request_io_size_bytes != disk_io_size_bytes\n",
    "mismatched_io_sizes = trace[trace[\"request_io_size_bytes\"] != trace[\"disk_io_size_bytes\"]]\n",
    "\n",
    "# Display the first few rows of mismatched IO sizes\n",
    "print(\"Rows where request_io_size_bytes != disk_io_size_bytes:\")\n",
    "mismatched_io_sizes.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
