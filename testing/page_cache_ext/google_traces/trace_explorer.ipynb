{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs\n",
    "\n",
    "cluster_name = \"cluster1_16TB\"\n",
    "date = \"20240115\"\n",
    "data_file = \"data-00000-of-00100\"\n",
    "url_template = \"https://storage.googleapis.com/thesios-io-traces/%s/%s/%s\"\n",
    "initialize_data_dir = True\n",
    "data_dir = \"data_%s_%s_%s\" % (cluster_name, date, data_file)\n",
    "output_trace_file = \"trace_%s_%s_%s.txt\" % (cluster_name, date, data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_to_file(url, filename):\n",
    "    response = requests.get(url)\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "def format_bytes(bytes: int) -> str:\n",
    "    for unit in [\"B\", \"KB\", \"MB\", \"GB\", \"TB\"]:\n",
    "        if bytes < 1024:\n",
    "            return f\"{bytes:.2f} {unit}\"\n",
    "        bytes /= 1024\n",
    "    return f\"{bytes:.2f} PB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "local_file = \"%s_%s_%s.csv\" % (cluster_name, date, data_file)\n",
    "download_url = url_template % (cluster_name, date, data_file)\n",
    "\n",
    "if not os.path.exists(local_file):\n",
    "    print(\"Downloading %s to %s\" % (download_url, local_file))\n",
    "    download_to_file(download_url, local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d206e7d484caba36fa936dffa618118382e36e0a616a41...</td>\n",
       "      <td>2453358</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705305576</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>3390</td>\n",
       "      <td>3390</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0c44a2ae48ee5d8b0c299ee04f3071e53b7d95b61cc13c...</td>\n",
       "      <td>7354368</td>\n",
       "      <td>4aed9945dd146966ceb9894bafd139b178af74e984c59c...</td>\n",
       "      <td>1705167112</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1052672</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.004246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d206e7d484caba36fa936dffa618118382e36e0a616a41...</td>\n",
       "      <td>2456748</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705305576</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>12294</td>\n",
       "      <td>12294</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>95f38d49a2da49d63a4bd459b79e3d68c24e1e13878dd1...</td>\n",
       "      <td>417618</td>\n",
       "      <td>51d511367f2bdd00717fc55ff6db252e785fb629018a8b...</td>\n",
       "      <td>1705305525</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>95f38d49a2da49d63a4bd459b79e3d68c24e1e13878dd1...</td>\n",
       "      <td>417618</td>\n",
       "      <td>51d511367f2bdd00717fc55ff6db252e785fb629018a8b...</td>\n",
       "      <td>1705305525</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>WRITE</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "      <td>904237</td>\n",
       "      <td>904237</td>\n",
       "      <td>0</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000371</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            filename  file_offset  \\\n",
       "0  d206e7d484caba36fa936dffa618118382e36e0a616a41...      2453358   \n",
       "1  0c44a2ae48ee5d8b0c299ee04f3071e53b7d95b61cc13c...      7354368   \n",
       "2  d206e7d484caba36fa936dffa618118382e36e0a616a41...      2456748   \n",
       "3  95f38d49a2da49d63a4bd459b79e3d68c24e1e13878dd1...       417618   \n",
       "4  95f38d49a2da49d63a4bd459b79e3d68c24e1e13878dd1...       417618   \n",
       "\n",
       "                                         application      c_time io_zone  \\\n",
       "0                                           bigtable  1705305576    WARM   \n",
       "1  4aed9945dd146966ceb9894bafd139b178af74e984c59c...  1705167112    WARM   \n",
       "2                                           bigtable  1705305576    WARM   \n",
       "3  51d511367f2bdd00717fc55ff6db252e785fb629018a8b...  1705305525    WARM   \n",
       "4  51d511367f2bdd00717fc55ff6db252e785fb629018a8b...  1705305525    WARM   \n",
       "\n",
       "  redundancy_type op_type        service_class  from_flash_cache  cache_hit  \\\n",
       "0      REPLICATED   WRITE  THROUGHPUT_ORIENTED                 0         -1   \n",
       "1      REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "2      REPLICATED   WRITE  THROUGHPUT_ORIENTED                 0         -1   \n",
       "3      REPLICATED   WRITE  THROUGHPUT_ORIENTED                 0         -1   \n",
       "4      REPLICATED   WRITE  THROUGHPUT_ORIENTED                 0         -1   \n",
       "\n",
       "   request_io_size_bytes  disk_io_size_bytes  response_io_size_bytes  \\\n",
       "0                   3390                3390                       0   \n",
       "1                1050624             1052672                 1050624   \n",
       "2                  12294               12294                       0   \n",
       "3                      0                   0                       0   \n",
       "4                 904237              904237                       0   \n",
       "\n",
       "     start_time  disk_time  simulated_disk_start_time  simulated_latency  \n",
       "0  1.705306e+09   0.000000               0.000000e+00           0.000125  \n",
       "1  1.705306e+09   0.004239               1.705306e+09           0.004246  \n",
       "2  1.705306e+09   0.000000               0.000000e+00           0.000091  \n",
       "3  1.705306e+09   0.000000               0.000000e+00           0.000089  \n",
       "4  1.705306e+09   0.000000               0.000000e+00           0.000371  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace = pd.read_csv(local_file)\n",
    "trace.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of lines in trace: 168590\n"
     ]
    }
   ],
   "source": [
    "# How many lines is the trace?\n",
    "print(\"Number of lines in trace: %d\" % len(trace))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique filenames: 13565\n",
      "\n",
      "Unique filenames:\n",
      "['d206e7d484caba36fa936dffa618118382e36e0a616a4117f4d13d06b7dc7fe6'\n",
      " '0c44a2ae48ee5d8b0c299ee04f3071e53b7d95b61cc13c4289c11a56b5250a36'\n",
      " '95f38d49a2da49d63a4bd459b79e3d68c24e1e13878dd19aedfe0e2832c17d6e' ...\n",
      " '1f2ac552b5b9935abf7ef5df672e5da2e12b551fa63370baf3658f5c1b9876d2'\n",
      " '551dc659b6aa5b2b16c6267643236fb6746575f66d467a82fca72b51b1922c06'\n",
      " 'db132f3a38568dd6e1147bf081b02c1e3a3e7b061fa1a9887ba4f2e24b20a95d']\n"
     ]
    }
   ],
   "source": [
    "num_unique_filenames = trace[\"filename\"].nunique()\n",
    "print(f\"Number of unique filenames: {num_unique_filenames}\")\n",
    "print(\"\\nUnique filenames:\")\n",
    "print(trace[\"filename\"].unique())\n",
    "if num_unique_filenames > 15000:\n",
    "    raise Exception(\"Too many unique filenames: %d\" % num_unique_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size initialized: 65.21 GB\n",
      "Total size read: 53.07 GB\n",
      "Total size written: 18.92 GB\n"
     ]
    }
   ],
   "source": [
    "def process_trace(trace):\n",
    "    # We want to process the trace, operation by operation.\n",
    "    # Let's simplify: assume everything pre-exists, and we only need to keep track of the last read/write offset for each file.\n",
    "\n",
    "    # We will keep track of the last read offset for each file.\n",
    "    # Initialize dictionaries to store file metadata\n",
    "    file_metadata = {}\n",
    "\n",
    "    for _, row in trace.iterrows():\n",
    "        filename = row[\"filename\"]\n",
    "        offset = row[\"file_offset\"]\n",
    "        io_size = row[\"request_io_size_bytes\"]\n",
    "        operation = row[\"op_type\"]\n",
    "\n",
    "        if filename not in file_metadata:\n",
    "            file_metadata[filename] = {\n",
    "                \"max_read_offset\": 0,\n",
    "                \"max_write_offset\": 0,\n",
    "                \"max_initialized_offset\": 0,\n",
    "            }\n",
    "\n",
    "        metadata = file_metadata[filename]\n",
    "\n",
    "        if operation == \"READ\":\n",
    "            metadata[\"max_initialized_offset\"] = max(metadata[\"max_initialized_offset\"], offset + io_size)\n",
    "            metadata[\"max_read_offset\"] = max(metadata[\"max_read_offset\"], offset + io_size)\n",
    "        elif operation == \"WRITE\":\n",
    "            metadata[\"max_initialized_offset\"] = max(metadata[\"max_initialized_offset\"], offset + io_size)\n",
    "            metadata[\"max_write_offset\"] = max(metadata[\"max_write_offset\"], offset + io_size)\n",
    "\n",
    "    return file_metadata\n",
    "\n",
    "def print_file_metadata_stats(file_metadata):\n",
    "    total_size_initialized = sum([metadata[\"max_initialized_offset\"] for metadata in file_metadata.values()])\n",
    "    total_size_read = sum([metadata[\"max_read_offset\"] for metadata in file_metadata.values()])\n",
    "    total_size_written = sum([metadata[\"max_write_offset\"] for metadata in file_metadata.values()])\n",
    "\n",
    "    print(f\"Total size initialized: {format_bytes(total_size_initialized)}\")\n",
    "    print(f\"Total size read: {format_bytes(total_size_read)}\")\n",
    "    print(f\"Total size written: {format_bytes(total_size_written)}\")\n",
    "    return total_size_initialized, total_size_read, total_size_written\n",
    "\n",
    "\n",
    "file_metadata = process_trace(trace)\n",
    "total_size_initialized, total_size_read, total_size_written = print_file_metadata_stats(file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "GiB = 2 ** 30\n",
    "if total_size_initialized > 100 * GiB:\n",
    "    raise Exception(\"Too much data initialized: %s\" % format_bytes(total_size_initialized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data directory: data_cluster1_16TB_20240115_data-00000-of-00100\n",
      "Data directory already exists, not doing anything.\n"
     ]
    }
   ],
   "source": [
    "# Initialize a new directory and write the files needed to run the trace\n",
    "\n",
    "def initialize_file(file_path: str, size: int):\n",
    "    \"\"\"Initialize a file with pseudo-random data.\"\"\"\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        # Write data in chunks to avoid excessive memory usage\n",
    "        chunk_size = 16 * 1024 * 1024  # 16 MB chunks\n",
    "        remaining_size = size\n",
    "\n",
    "        while remaining_size > 0:\n",
    "            write_size = min(chunk_size, remaining_size)\n",
    "            data = np.random.bytes(write_size)\n",
    "            f.write(data)\n",
    "            remaining_size -= write_size\n",
    "    # print(f\"Initialized file: {file_path} with size: {format_bytes(size)}\")\n",
    "\n",
    "def initialize_file_single_arg(args):\n",
    "    return initialize_file(*args)\n",
    "\n",
    "def create_trace_data_dir(data_dir, file_metadata):\n",
    "    print(\"Initializing data directory: %s\" % data_dir)\n",
    "    if not os.path.exists(data_dir):\n",
    "        os.makedirs(data_dir)\n",
    "    else:\n",
    "        print(\"Data directory already exists, not doing anything.\")\n",
    "        return\n",
    "\n",
    "    # Process files in parallel\n",
    "    total_files = len(file_metadata)\n",
    "    print(f\"Initializing {total_files} files...\")\n",
    "\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        tasks = []\n",
    "        for filename, metadata in file_metadata.items():\n",
    "            file_path = os.path.join(data_dir, filename)\n",
    "            size = metadata[\"max_initialized_offset\"]\n",
    "            tasks.append((file_path, size))\n",
    "\n",
    "        # Use multiprocessing to parallelize file initialization\n",
    "        for _ in tqdm(pool.imap(initialize_file_single_arg, tasks), total=total_files):\n",
    "            pass\n",
    "\n",
    "    print(\"File initialization complete.\")\n",
    "\n",
    "\n",
    "\n",
    "if initialize_data_dir:\n",
    "    create_trace_data_dir(data_dir, file_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting trace to benchmark client format...\n",
      "Trace file converted to benchmark client format: trace_cluster1_16TB_20240115_data-00000-of-00100.txt\n"
     ]
    }
   ],
   "source": [
    "# Format the trace file to be used in the benchmark client\n",
    "# The expected format is:\n",
    "# <op_type> <filename>:<offset>:<size>\n",
    "# Example:\n",
    "# READ file1:0:4096\n",
    "# WRITE file2:0:4096\n",
    "\n",
    "def convert_trace_to_bench_client_format(trace: pd.DataFrame, output_file: str):\n",
    "    with open(output_file, \"w\") as f:\n",
    "        for _, row in trace.iterrows():\n",
    "            filename = row[\"filename\"]\n",
    "            offset = row[\"file_offset\"]\n",
    "            io_size = row[\"request_io_size_bytes\"]\n",
    "            operation = row[\"op_type\"]\n",
    "\n",
    "            f.write(f\"{operation} {filename}:{offset}:{io_size}\\n\")\n",
    "\n",
    "if not os.path.exists(output_trace_file):\n",
    "    print(\"Converting trace to benchmark client format...\")\n",
    "    convert_trace_to_bench_client_format(trace, output_trace_file)\n",
    "    print(\"Trace file converted to benchmark client format: %s\" % output_trace_file)\n",
    "else:\n",
    "    print(\"Trace file already exists: %s\" % output_trace_file)\n",
    "    print(\"Not overwriting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows where request_io_size_bytes != disk_io_size_bytes:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>file_offset</th>\n",
       "      <th>application</th>\n",
       "      <th>c_time</th>\n",
       "      <th>io_zone</th>\n",
       "      <th>redundancy_type</th>\n",
       "      <th>op_type</th>\n",
       "      <th>service_class</th>\n",
       "      <th>from_flash_cache</th>\n",
       "      <th>cache_hit</th>\n",
       "      <th>request_io_size_bytes</th>\n",
       "      <th>disk_io_size_bytes</th>\n",
       "      <th>response_io_size_bytes</th>\n",
       "      <th>start_time</th>\n",
       "      <th>disk_time</th>\n",
       "      <th>simulated_disk_start_time</th>\n",
       "      <th>simulated_latency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0c44a2ae48ee5d8b0c299ee04f3071e53b7d95b61cc13c...</td>\n",
       "      <td>7354368</td>\n",
       "      <td>4aed9945dd146966ceb9894bafd139b178af74e984c59c...</td>\n",
       "      <td>1705167112</td>\n",
       "      <td>WARM</td>\n",
       "      <td>REPLICATED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1052672</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.004246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ccc5dd26c7981bb3805b340632295fcf6abae62d81ce8c...</td>\n",
       "      <td>0</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1705304559</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1052672</td>\n",
       "      <td>1050624</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.031058</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.067846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dbc19080ff913d52ed5bd386c87ac2aaddeef34aae9a66...</td>\n",
       "      <td>5484544</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705056906</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>540672</td>\n",
       "      <td>536576</td>\n",
       "      <td>540672</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.061153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>dbc19080ff913d52ed5bd386c87ac2aaddeef34aae9a66...</td>\n",
       "      <td>6021120</td>\n",
       "      <td>bigtable</td>\n",
       "      <td>1705056906</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139264</td>\n",
       "      <td>135168</td>\n",
       "      <td>139264</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.000961</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.045035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4ffe32bf2b6ed9e5970687ce370ace496e352b099e191f...</td>\n",
       "      <td>1050624</td>\n",
       "      <td>eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...</td>\n",
       "      <td>1704572620</td>\n",
       "      <td>WARM</td>\n",
       "      <td>ERASURE_CODED</td>\n",
       "      <td>READ</td>\n",
       "      <td>THROUGHPUT_ORIENTED</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>411648</td>\n",
       "      <td>413696</td>\n",
       "      <td>411648</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>1.705306e+09</td>\n",
       "      <td>0.045428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             filename  file_offset  \\\n",
       "1   0c44a2ae48ee5d8b0c299ee04f3071e53b7d95b61cc13c...      7354368   \n",
       "7   ccc5dd26c7981bb3805b340632295fcf6abae62d81ce8c...            0   \n",
       "10  dbc19080ff913d52ed5bd386c87ac2aaddeef34aae9a66...      5484544   \n",
       "11  dbc19080ff913d52ed5bd386c87ac2aaddeef34aae9a66...      6021120   \n",
       "12  4ffe32bf2b6ed9e5970687ce370ace496e352b099e191f...      1050624   \n",
       "\n",
       "                                          application      c_time io_zone  \\\n",
       "1   4aed9945dd146966ceb9894bafd139b178af74e984c59c...  1705167112    WARM   \n",
       "7   eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1705304559    WARM   \n",
       "10                                           bigtable  1705056906    WARM   \n",
       "11                                           bigtable  1705056906    WARM   \n",
       "12  eccab0ec807ba5e9c86ea4d72b7272534653995c86e7d3...  1704572620    WARM   \n",
       "\n",
       "   redundancy_type op_type        service_class  from_flash_cache  cache_hit  \\\n",
       "1       REPLICATED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "7    ERASURE_CODED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "10   ERASURE_CODED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "11   ERASURE_CODED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "12   ERASURE_CODED    READ  THROUGHPUT_ORIENTED                 0          0   \n",
       "\n",
       "    request_io_size_bytes  disk_io_size_bytes  response_io_size_bytes  \\\n",
       "1                 1050624             1052672                 1050624   \n",
       "7                 1050624             1052672                 1050624   \n",
       "10                 540672              536576                  540672   \n",
       "11                 139264              135168                  139264   \n",
       "12                 411648              413696                  411648   \n",
       "\n",
       "      start_time  disk_time  simulated_disk_start_time  simulated_latency  \n",
       "1   1.705306e+09   0.004239               1.705306e+09           0.004246  \n",
       "7   1.705306e+09   0.031058               1.705306e+09           0.067846  \n",
       "10  1.705306e+09   0.002441               1.705306e+09           0.061153  \n",
       "11  1.705306e+09   0.000961               1.705306e+09           0.045035  \n",
       "12  1.705306e+09   0.002005               1.705306e+09           0.045428  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find lines where request_io_size_bytes != disk_io_size_bytes\n",
    "mismatched_io_sizes = trace[trace[\"request_io_size_bytes\"] != trace[\"disk_io_size_bytes\"]]\n",
    "\n",
    "# Display the first few rows of mismatched IO sizes\n",
    "print(\"Rows where request_io_size_bytes != disk_io_size_bytes:\")\n",
    "mismatched_io_sizes.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
